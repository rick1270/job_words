{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"QpDJ10COZoR3"},"outputs":[],"source":["from datetime import datetime as dt\n","import requests\n","import glob\n","import json\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import spacy\n","import urllib3\n","import time\n","import traceback\n","import os\n","from dotenv import load_dotenv\n","from itertools import chain\n","import ast\n","\n","# df = pd.read_pickle('Data_Words/Data/job_words_indeed.pkl')\n","# description_sentences = df['description_sentences']\n","now = int(time.time() * 1000000)\n","load_dotenv(\"Data_Words/.env\")\n","scraper_api = os.getenv(\"API_KEY\")\n","from pathlib import Path\n","import numpy as np\n","from spacy.matcher import Matcher\n","\n","cwd = Path.cwd()\n","nlp = spacy.load(\"en_core_web_lg\")\n","stopwords = list(nlp.Defaults.stop_words)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"0ffvD3ihZoR4"},"source":["#### Names and Types\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"qnfLNrLlZoR4"},"outputs":[],"source":["def url_next(keyword, location, days, last_id=None, page_count=0):\n","    \"\"\"Create start url for ids if lastId = None.  Otherwise uses id from previous\n","    request\"\"\"\n","    url = f\"https://www.indeed.com/jobs?q={keyword}&l={location}&radius=15&sort=date&fromage={days}\"\n","    if page_count == 0:\n","        return url\n","    else:\n","        return f\"{url}&start={page_count * 10}&vjk={last_id}\"\n","\n","\n","# u = url_next('data', 'San+Diego+CA', 14, last_id=None, page_count=0)\n","# 'https://www.indeed.com/jobs?q=data&l=San+Diego+CA&radius=15&sort=date&fromage=14'\n","\n","\n","def file_name_ids(folder_path, keyword, location):\n","    \"\"\"creates json file name for ids using keyword and location\"\"\"\n","    file_name = f\"{folder_path}/indeedIds_{keyword}_{location}.json\"\n","    return file_name"]},{"cell_type":"markdown","metadata":{"id":"9zGoEyg_ZoR5"},"source":["#### Storing and Retrieving\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"mRUakOo0ZoR5"},"outputs":[],"source":["def dd(any_list):\n","    \"\"\"Converts list to set then back to list for the purpose of removing duplicates\"\"\"\n","    list_to_set = set(any_list)\n","    return list(list_to_set)\n","\n","\n","def j_load(file_name):\n","    \"\"\"Retreives existing ids or creates new file.  Returns tuple with id list and count.\n","    Dedupes as precaution.\"\"\"\n","    try:\n","        with open(file_name, \"r\", encoding=\"utf-8\") as f:\n","            ja = json.load(f)\n","        og_ids = ja[file_name]\n","        og_list_dd = dd(og_ids)\n","        print(f\"Starting with {len(og_list_dd)} existing ids for {file_name}\")\n","        return (len(og_list_dd), og_list_dd)\n","    except (FileNotFoundError, TypeError, KeyError, ValueError):\n","        print(f\"No file found using {file_name} starting fresh.\")\n","        return (0, [])\n","\n","\n","def j_dump(file_name, ids):\n","    \"\"\"saves ids as json\"\"\"\n","    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n","        json.dump({file_name: dd(ids)}, f)\n","    print(f\"{len(ids)} ids saved to {file_name}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"s-AAb_NeZoR5"},"source":["#### Scrape\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ObNKjTriZoR5"},"outputs":[],"source":["# Uses proxies and redirects to automatically scrape id html.  Requires key.\n","def scrape(retries, api_key, url):\n","    \"\"\"request and return html from job listing page with retries\n","    and results printed.  Single thread.  Returned html is used to\n","    make next url.  Returns r\"\"\"\n","    tries = 0\n","    while tries <= retries:\n","        scraper_api = api_key\n","        payload = {\"api_key\": scraper_api, \"url\": url}\n","        print(f\"Calling: {url}\")\n","        r = requests.get(\"http://api.scraperapi.com\", params=payload, timeout=30)\n","        try:\n","            if r.headers[\"sa-statusCode\"] == \"200\":\n","                print(\n","                    f\"{r.headers['Date']}   {r.headers['sa-final-url']} \\\n","                      status: {r.headers['sa-statusCode']}\"\n","                )\n","            return r\n","        except KeyError:\n","            print(f\"{r.headers}\")\n","            tries += 1\n","        print(rf\"Try {tries} of {retries} n\\ {r.headers}\")\n","    return print(f\"{url} has problems\")\n","\n","\n","def calling(keyword, location, days, api_key, last_id, page_count):\n","    \"\"\"Bundles methods and returns id html.\"\"\"\n","    id_url = url_next(keyword, location, days, last_id, page_count)\n","    r = scrape(3, api_key, id_url)\n","    return r\n","\n","\n","# scrape(retries=3, api_key=scraper_api, url='https://www.indeed.com/jobs?q=data&l=Tampa&radius=15&sort=date&fromage=7')"]},{"cell_type":"markdown","metadata":{"id":"dmK8LHwsZoR5"},"source":["#### Get Ids\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"bm_WXGzEZoR5"},"outputs":[],"source":["def get_ids(r):\n","    \"\"\"convert search page response to list of ids.\"\"\"\n","    soup = BeautifulSoup(r.text, \"html.parser\")\n","    td_soup = soup.find_all(\"h2\")\n","    jk_list = []\n","    for soup in td_soup:\n","        aa = soup.a\n","        try:\n","            jk = aa.attrs[\"data-jk\"]\n","            jk_list.append(jk)\n","        except AttributeError:\n","            pass\n","    return jk_list\n","\n","\n","def get_new_ids(r, file_name, old_ids):\n","    # takes allIds and pageCount from previous loop if available\n","    \"\"\"Returns a uniques set of ids consisting of old ids and this loop.\"\"\"\n","    # loads saved file if first loop\n","\n","    if len(old_ids) == 0:\n","        existing_ids_load = j_load(file_name)\n","        existing_id_count = existing_ids_load[0]\n","        existing_ids = existing_ids_load[1]\n","        existing_id_dd = dd(existing_ids)\n","        print(f\"starting id count is {existing_ids_load[0]}\")\n","    else:\n","        existing_id_dd = dd(old_ids)\n","        existing_id_count = len(existing_id_dd)\n","    try:\n","        loop_id_list = get_ids(r)\n","        all_unique = dd(existing_id_dd + loop_id_list)\n","    except (NameError, TypeError, AttributeError):\n","        all_unique = []\n","    all_unique_count = len(all_unique)\n","    loop_unique_count = all_unique_count - existing_id_count\n","    print(\n","        f\"\\n{dt.now()} page_unique_count: {loop_unique_count} currentCount: {all_unique_count}\"\n","    )\n","    return all_unique\n","\n","\n","def zero_count(loop_count):\n","    \"\"\"Returns false when 0 new ids returned twice in a row.\n","    Used to end scraping\"\"\"\n","    if len(loop_count) <= 2:\n","        return True\n","    if loop_count[-1] + loop_count[-2] > 0:\n","        return True\n","    return False\n","\n","\n","def just_ids(keyword=None, location=None, api_key=None, days=None, folder_path=None):\n","    \"\"\"Assembles methods for IDs.  Save and count all.  Keyboard interupt causes end and save\"\"\"\n","    current_id_list = dd([])\n","    loop_count_list = []\n","    page_count = 0\n","    file_name = file_name_ids(folder_path, keyword, location)\n","    # breaks loop when 2 consecutive loops return 0 new ids\n","    try:\n","        while zero_count(loop_count_list):\n","            try:\n","                last_id = current_id_list[-1]\n","            except IndexError:\n","                last_id = str()\n","            try:\n","                r = calling(keyword, location, days, api_key, last_id, page_count)\n","            except urllib3.exceptions.ReadTimeoutError as e:\n","                print(f\"{e}\")\n","                j_dump(file_name, current_id_list)\n","            newest_ids = get_new_ids(r, file_name, current_id_list)\n","            try:\n","                loop_count = len(newest_ids) - len(current_id_list)\n","            except TypeError:\n","                loop_count = len(newest_ids)\n","            loop_count_list.append(loop_count)\n","            page_count += 1\n","            for new in newest_ids:\n","                current_id_list.append(new)\n","    except KeyboardInterrupt:\n","        pass\n","    j_dump(file_name, dd(current_id_list))\n","    print(\n","        f\"Finished!  New id count is {len(dd(current_id_list))} \\\n","        Saved to: {file_name}\"\n","    )"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# ids_san_diego = just_ids(\n","#     keyword=\"data\",\n","#     location=\"San+Diego+CA\",\n","#     api_key=scraper_api,\n","#     days=7,\n","#     folder_path=\"Data_Words/Indeed_ids\",\n","# )"]},{"cell_type":"markdown","metadata":{"id":"5VccKrC0ZoR5"},"source":["#### Filter Ids for use\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"eAYwf3lAZoR6","outputId":"22100ea4-2dee-4b5f-80e2-56f099acafca"},"outputs":[],"source":["def get_df_ids(\n","    file=\"Data_Words/Data/job_words_indeed.pkl\",\n","):\n","    \"\"\"Retrieves main df from csv and returns the Job Id column as a list\"\"\"\n","    \"\"\"Used later to filter out ids that have already been processed\"\"\"\n","    try:\n","        df = pd.read_pickle(file)\n","        df_ids = list(df[\"job_id\"])\n","        return df_ids\n","    except FileNotFoundError:\n","        print(f\"no pkl found in {file}\")\n","        return [\"x\"]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def get_job_dict(id_tuple):\n","    \"\"\"Makes api call and converts html response to soup, extracts specific json, converts to dict return\"\"\"\n","    keyword = id_tuple[0]\n","    location = id_tuple[1]\n","    id = id_tuple[2]\n","    url = id_tuple[3]\n","    r = scrape(3, scraper_api, url).text\n","    try:\n","        # create soup from r\n","        soup = BeautifulSoup(r, \"html.parser\")\n","        # find and extract json from soup\n","        j_soup = soup.find(\"script\", type=\"application/ld+json\").text\n","        # convert json to dict\n","        d_soup = json.loads(j_soup)\n","        # Append data from tuple\n","        d_soup.update(\n","            {\n","                \"jobId\": id,\n","                \"jobKeyword\": keyword,\n","                \"jobSearchLocation\": location,\n","            }\n","        )\n","        # print(f'{dt.now()}get_job_dict try')\n","        return d_soup\n","    except AttributeError:\n","        # print(f'{dt.now()}get_job_dict ex')\n","        return {\"jobId\": id, \"jobKeyword\": keyword, \"jobSearchLocation\": location}\n","\n","\n","# test_get_job_dict = get_job_dict(test_tup)\n","# test_get_job_dict"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"G4j0l5zgZoR7","outputId":"1bbd114b-2651-436c-8c49-581ba8a896be"},"outputs":[],"source":["# Ids ready\n","def ids_warming(path):\n","    \"\"\"Extract ids from json files.  Ouput list of tuples with keyword, location, id, URL\"\"\"\n","    id_tuple_list = []\n","    # get ids that have already run\n","    df_ids = get_df_ids()\n","    # list of id file names\n","    ids = glob.glob(f\"{path}/indeedIds_*.json\")\n","    # loop through file names extract kw and location\n","    for id in ids:\n","        id_path = id.split(\"/\")[-1]\n","        id_name = id_path.split(\"_\")\n","        kw = id_name[1]\n","        loc = id_name[2].split(\".\")[0]\n","        # open each id file\n","        with open(id, \"r\", encoding=\"utf-8\") as f:\n","            j_ids = json.load(f)[id]\n","            # if id already in df skip\n","            for j in j_ids:\n","                if j in df_ids:\n","                    pass\n","                elif kw in (\"test\", \"write\"):\n","                    pass\n","                # not in df... create tup using (kw,loc,id) and append to list\n","                else:\n","                    url = f\"https://www.indeed.com/m/viewjob?jk={j}\"\n","                    id_tup = (kw, loc, j, url)\n","                    id_tuple_list.append(id_tup)\n","    # return set of all unique unprocessed tuples\n","    tuple_dd_list = dd(id_tuple_list)\n","    print(f\"{len(tuple_dd_list)} unique job ids will be processed\")\n","    return tuple_dd_list"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Fob64m_7ZoR8"},"outputs":[],"source":["# This func does not work as stand alone\n","def li_li_li_list(description):\n","    \"\"\"Takes description dict and returns li values as list\"\"\"\n","    # Resoup dict\n","    d_soup = BeautifulSoup(description, \"html.parser\")\n","    # get list of li tag text\n","    lis = d_soup.find_all(\"li\")\n","    tagList = []\n","    for l in lis:\n","        # get just li text and put back in list\n","        bullet = l.text\n","        tagList.append(bullet)\n","    # print(f'{dt.now()}li_li_li_list')\n","    return tagList"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"u-vbjy4oZoR9"},"outputs":[],"source":["# test_sen = test_check['description_list']\n","\n","\n","def spacy_proper(doc):\n","    \"\"\"Takes string as input and returns a list of Proper Nouns\"\"\"\n","    pn_list = []\n","    for tok in doc:\n","        if tok.pos_ == \"PROPN\":\n","            pn_list.append(tok.text)\n","        else:\n","            pass\n","    return pn_list\n","\n","\n","def sentence_parse_proper(sentences):\n","    \"\"\"Parses list of sentences and returns list of proper nouns\"\"\"\n","    col_lists = []\n","    try:\n","        for sentence in sentences:\n","            ss = sentence.strip()\n","            doc = nlp(ss)\n","            pn = spacy_proper(doc)\n","            col_lists.append(pn)\n","    except ValueError:\n","        col_lists.append([])\n","    return set(chain.from_iterable(col_lists))"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"n9uc2NPAZoR8","outputId":"f48f9b93-663a-4ea7-a213-5afb2692c456"},"outputs":[],"source":["def pattern_lower(csv_file, column_name):\n","    \"\"\"Formats column from csv file into patterns for matching\"\"\"\n","    pattern_list = []\n","    df = pd.read_csv(csv_file)\n","    word_list = list(df[column_name])\n","    clean_word_list = [str(t).lower().strip() for t in word_list if t is not np.nan]\n","    split_list = [t.split() for t in clean_word_list]\n","    for i in range(len(split_list)):\n","        words = []\n","        sentence = split_list[i]\n","        for w in sentence:\n","            pattern = dict(LOWER=str(w))\n","            words.append(pattern)\n","        pattern_list.append(words)\n","    # print(f'{column_name} now contains {len(pattern_list)} keywords')\n","    return pattern_list\n","\n","\n","def data_word_match(sentence, csv_file, column_name):\n","    matcher = Matcher(nlp.vocab)\n","    word_patterns = pattern_lower(csv_file, column_name)\n","    matcher.add(column_name, word_patterns, greedy=\"FIRST\")\n","    doc = nlp(sentence)\n","    matches = matcher(doc)\n","    words = []\n","    for match_id, start, end in matches:\n","        span = doc[start:end]\n","        words.append(span.text)\n","    return list(words)\n","\n","\n","## TODO use lemmatization to match base words instead of exact"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def sentence_parse_data_words(sentences, csv_file, column_name):\n","    \"\"\"Takes string as input and returns a list of Proper Nouns\"\"\"\n","    words_lists = []\n","    try:\n","        for sentence in sentences:\n","            words = data_word_match(sentence, csv_file, column_name)\n","            words_lists.append(words)\n","    except ValueError:\n","        words_lists.append([])\n","    return set(chain.from_iterable(words_lists))\n","\n","\n","#### Extract and Catch\n","\n","\n","def check_and_extract(full_dict):\n","    \"\"\"This function is designed to catch errors in the dictionary.\n","    Function li_li_li_list extracts list of values.  Dict with data and lables returned\n","    \"\"\"\n","    try:\n","        company = full_dict[\"hiringOrganization\"][\"name\"]\n","    except KeyError:\n","        company = \"Unavailable\"\n","    except TypeError:\n","        company = \"Unavailable\"\n","    try:\n","        title = full_dict[\"title\"]\n","    except TypeError:\n","        title = \"Unavailable\"\n","    except KeyError:\n","        title = \"Unavailable\"\n","    try:\n","        base_salary_low = full_dict[\"baseSalary\"][\"value\"][\"minValue\"]\n","    except KeyError:\n","        base_salary_low = \"Unavailable\"\n","    except TypeError:\n","        base_salary_low = \"Unavailable\"\n","    try:\n","        base_salary_high = full_dict[\"baseSalary\"][\"value\"][\"maxValue\"]\n","    except KeyError:\n","        base_salary_high = \"Unavailable\"\n","    except TypeError:\n","        base_salary_high = \"Unavailable\"\n","    try:\n","        salary_period = full_dict[\"baseSalary\"][\"value\"][\"unitText\"]\n","    except KeyError:\n","        salary_period = \"Unavailable\"\n","    except TypeError:\n","        salary_period = \"Unavailable\"\n","    try:\n","        date_posted = full_dict[\"datePosted\"]\n","    except KeyError:\n","        date_posted = \"Unavailable\"\n","    except TypeError:\n","        date_posted = \"Unavailable\"\n","    try:\n","        date_expires = full_dict[\"date_expires\"]\n","    except TypeError:\n","        date_expires = \"Unavailable\"\n","    except KeyError:\n","        date_expires = \"Unavailable\"\n","    try:\n","        employment_type = full_dict[\"employmentType\"]\n","    except KeyError:\n","        employment_type = \"Unavailable\"\n","    except TypeError:\n","        employment_type = \"Unavailable\"\n","    try:\n","        location = full_dict[\"jobLocation\"][\"address\"][\"addressLocality\"]\n","    except KeyError:\n","        location = \"Unavailable\"\n","    except TypeError:\n","        location = \"Unavailable\"\n","    try:\n","        description_raw = full_dict[\"description\"]\n","    except KeyError:\n","        description_raw = \"Unavailable\"\n","    except TypeError:\n","        description_raw = \"Unavailable\"\n","    try:\n","        description_list = li_li_li_list(description_raw)\n","    except KeyError:\n","        description_list = \"Unavailable\"\n","    except TypeError:\n","        description_list = \"Unavailable\"\n","    skills = sentence_parse_data_words(\n","        description_list, \"Data/snow_words.csv\", \"Data_Skills\"\n","    )\n","    data_skills = [x.lower() for x in skills]\n","    technology = sentence_parse_data_words(\n","        description_list, \"Data/snow_words.csv\", \"Data_Technology\"\n","    )\n","    data_technology = [x.lower() for x in technology]\n","    propers = sentence_parse_proper(description_list)\n","    proper_nouns = [\n","        x.lower()\n","        for x in propers\n","        if x.lower() not in data_skills and x.lower() not in data_technology\n","    ]\n","    job_id = full_dict[\"jobId\"]\n","    keyword = full_dict[\"jobKeyword\"]\n","    search_location = full_dict[\"jobSearchLocation\"]\n","    print(\n","        f\"{job_id}  {keyword}   {search_location}  {company}  {title}  {date_posted}\\\n","          {employment_type}  {location}  \\n  {proper_nouns} \\n {data_skills} \\n {data_technology}\"\n","    )\n","    job_dict = {\n","        \"job_id\": job_id,\n","        \"entered\": time.strftime(\"%D %T\"),\n","        \"search_keyword\": keyword,\n","        \"search_location\": search_location,\n","        \"job_company\": company,\n","        \"job_title\": title,\n","        \"job_date_posted\": date_posted,\n","        \"job_date_expires\": date_expires,\n","        \"pay_low\": base_salary_low,\n","        \"pay_high\": base_salary_high,\n","        \"pay_period\": salary_period,\n","        \"job_type\": employment_type,\n","        \"job_location\": location,\n","        \"description_sentences\": description_list,\n","        \"proper_nouns\": proper_nouns,\n","        \"data_skills\": data_skills,\n","        \"data_technology\": data_technology,\n","    }\n","    return job_dict\n","\n","\n","# test_check = check_and_extract(test_get_job_dict)\n","# test_check"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def come_together(path, table_name):\n","    \"\"\"runs scraped ids and returns list df\"\"\"\n","    dict_list = []\n","    tup_list = ids_warming(\n","        path, table_name\n","    )  # returns list of id tuples  ->list\n","    er_count = 0\n","    try:\n","        for tup in tup_list:\n","            try:\n","                job_dict = get_job_dict(\n","                    tup\n","                )  # takes one id tuple and outputs dict with data -> dict\n","                new_job_dict = check_and_extract(\n","                    job_dict\n","                )  #  (description_list:[]) -> dict\n","                # should only run ids not already in table\n","                dict_list.append(new_job_dict)\n","                print(f\"{len(dict_list)} jobs out of {len(tup_list)} processed\")\n","            except TimeoutError:\n","                traceback.print_exc()\n","                er_count += 1\n","                er_retry = er_count * 60\n","                print(f\"Error has occured.  Will retry in {er_retry}\")\n","        df_new = pd.DataFrame(dict_list)\n","        df_to_table(df_new, table_name)\n","        return df_new\n","    except KeyboardInterrupt:\n","        df_new = pd.DataFrame(dict_list)\n","        df_to_table(df_new, table_name)\n","        return df_new"]},{"cell_type":"markdown","metadata":{"id":"BZhDhsp6ZoR8"},"source":["#### Get and Clean up words\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"K-bnNG-HZoR9"},"outputs":[],"source":["def come_together():\n","    \"\"\"runs scraped ids and returns list df\"\"\"\n","    dict_list = []\n","    tup_list = ids_warming(path=\"Data_Words/Data\")  # returns list of id tuples  ->list\n","    er_count = 0\n","    try:\n","        for tup in tup_list:\n","            try:\n","                job_dict = get_job_dict(\n","                    tup\n","                )  # takes one id tuple and outputs dict with data -> dict\n","                new_job_dict = check_and_extract(\n","                    job_dict\n","                )  # Takes description key from job dict and converts to list (description_list:[]) -> dict\n","                dict_list.append(new_job_dict)\n","                print(f\"{len(dict_list)} jobs out of {len(tup_list)} processed\")\n","            except TimeoutError:\n","                traceback.print_exc()\n","                er_count += 1\n","                er_retry = er_count * 60\n","                print(f\"Error has occured.  Will retry in {er_retry}\")\n","        df_new = pd.DataFrame(dict_list)\n","        df_main = pd.read_pickle(\"Data_Words/Data/job_words_indeed.pkl\")\n","        combo_df = pd.concat([df_main, df_new], ignore_index=True)\n","        combo_df.to_pickle(f\"Data_Words/Data/job_words_indeed.pkl\")\n","        combo_df.to_pickle(f\"Data_Words/BackUps/job_words_indeed{now}.pkl\")\n","        return combo_df\n","    except KeyboardInterrupt:\n","        df_new = pd.DataFrame(dict_list)\n","        df_main = pd.read_pickle(\"Data_Words/Data/job_words_indeed.pkl\")\n","        combo_df = pd.concat([df_main, df_new], ignore_index=True)\n","        combo_df.to_pickle(f\"Data_Words/Data/job_words_indeed.pkl\")\n","        combo_df.to_pickle(f\"Data_Words/BackUps/job_words_indeed{now}.pkl\")\n","        return df_new"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"vZatxznhZoR9","outputId":"b71e6b22-b2c4-4cfb-b7c3-c608e9c06e90"},"outputs":[{"name":"stdout","output_type":"stream","text":["no pkl found in Data_Words/Data/job_words_indeed.pkl\n","0 unique job ids will be processed\n"]},{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'Data_Words/Data/job_words_indeed.pkl'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ids_remote = just_ids(keyword='snowflake', location='Remote', api_key=scraper_api, days=14,\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#              folder_path='Data_Words/Data')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ids_san_diego = just_ids(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     folder_path=\"Data_Words/Data\",\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m df \u001b[38;5;241m=\u001b[39m come_together()\n","Cell \u001b[0;32mIn[17], line 23\u001b[0m, in \u001b[0;36mcome_together\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError has occured.  Will retry in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mer_retry\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m df_new \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(dict_list)\n\u001b[0;32m---> 23\u001b[0m df_main \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData_Words/Data/job_words_indeed.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m combo_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_main, df_new], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m combo_df\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData_Words/Data/job_words_indeed.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/pickle.py:189\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m    190\u001b[0m     filepath_or_buffer,\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    192\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    193\u001b[0m     is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    194\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    195\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data_Words/Data/job_words_indeed.pkl'"]}],"source":["# ids_remote = just_ids(keyword='snowflake', location='Remote', api_key=scraper_api, days=14,\n","#              folder_path='Data_Words/Data')\n","# ids_san_diego = just_ids(\n","#     keyword=\"snowflake\",\n","#     location=\"San+Diego+CA\",\n","#     api_key=scraper_api,\n","#     days=14,\n","#     folder_path=\"Data_Words/Data\",\n","# )\n","# ids_tampa = just_ids(\n","#     keyword=\"snowflake\",\n","#     location=\"Tampa\",\n","#     api_key=scraper_api,\n","#     days=14,\n","#     folder_path=\"Data_Words/Data\",\n","# )\n","\n","df = come_together()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Success!  280 rows added to job_words_raw in 1 chunks\n"]}],"source":["from snot import snowy\n","\n","connection_parameters = {\n","    \"user\": \"Kona\",\n","    \"password\": \"ngFWnMT9cuG6XM6\",\n","    \"account\": \"bepcfpi-gf10139\",\n","    \"role\": \"ACCOUNTADMIN\",\n","    \"warehouse\": \"LOADER\",\n","    \"database\": \"PC_DBT_DB\",\n","    \"schema\": \"RAW\",\n","}\n","\n","\n","snowy.df_to_table(df, \"job_words_raw\", connection_parameters)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"17OaKCFl4DX9HQFmyRXjC2JvtPEWhe7Dj","timestamp":1711148541539}]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}
