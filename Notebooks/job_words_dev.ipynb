{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names\n",
    "#\n",
    "\"\"\"create file and url names\"\"\"\n",
    "\n",
    "\n",
    "def url_next(keyword, location, days, last_id=None, page_count=0):\n",
    "    \"\"\"Create start url for ids if lastId = None.  Otherwise uses id from previous request\"\"\"\n",
    "    url = f\"https://www.indeed.com/jobs?q={keyword}&l={location}&radius=15&sort=date&fromage={days}\"\n",
    "    if page_count == 0:\n",
    "        return url\n",
    "    else:\n",
    "        return f\"{url}&start={page_count * 10}&vjk={last_id}\"\n",
    "\n",
    "\n",
    "def file_name_ids(folder_path, keyword, location):\n",
    "    \"\"\"creates json file name for ids using keyword and location\"\"\"\n",
    "    file_name = f\"{folder_path}/indeedIds_{keyword}_{location}.json\"\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_retrieve\n",
    "\n",
    "\"\"\"Stores and retrieves local files\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def dd(any_list):\n",
    "    \"\"\"Converts list to set then back to list for the purpose of removing duplicates\"\"\"\n",
    "    list_to_set = set(any_list)\n",
    "    return list(list_to_set)\n",
    "\n",
    "\n",
    "def j_load(file_name):\n",
    "    \"\"\"Retreives existing ids or creates new file.  Returns tuple with id list and count.\n",
    "    Dedupes as precaution.\"\"\"\n",
    "    try:\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "            ja = json.load(f)\n",
    "        og_ids = ja[file_name]\n",
    "        og_list_dd = dd(og_ids)\n",
    "        print(f\"Starting with {len(og_list_dd)} existing ids for {file_name}\")\n",
    "        return (len(og_list_dd), og_list_dd)\n",
    "    except (FileNotFoundError, TypeError, KeyError, ValueError):\n",
    "        print(f\"No file found using {file_name} starting fresh.\")\n",
    "        return (0, [])\n",
    "\n",
    "\n",
    "def j_dump(file_name, ids):\n",
    "    \"\"\"saves ids as json\"\"\"\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({file_name: dd(ids)}, f)\n",
    "    print(f\"{len(ids)} ids saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape\n",
    "#\n",
    "# \"\"\"Connection to api and scraping\"\"\"\n",
    "\n",
    "import requests\n",
    "import names\n",
    "\n",
    "\n",
    "# Uses proxies and redirects to automatically scrape id html.  Requires key.\n",
    "def scrape(retries, api_key, url):\n",
    "    \"\"\"request and return html from job listing page with retries\n",
    "    and results printed.  Single thread.  Returned html is used to\n",
    "    make next url.  Returns r\"\"\"\n",
    "    tries = 0\n",
    "    while tries <= retries:\n",
    "        scraper_api = api_key\n",
    "        payload = {\"api_key\": scraper_api, \"url\": url}\n",
    "        print(f\"Calling: {url}\")\n",
    "        r = requests.get(\"http://api.scraperapi.com\", params=payload, timeout=30)\n",
    "        try:\n",
    "            if r.headers[\"sa-statusCode\"] == \"200\":\n",
    "                print(\n",
    "                    f\"{r.headers['Date']}   {r.headers['sa-final-url']} \\\n",
    "                      status: {r.headers['sa-statusCode']}\"\n",
    "                )\n",
    "            return r\n",
    "        except KeyError:\n",
    "            print(f\"{r.headers}\")\n",
    "            tries += 1\n",
    "        print(rf\"Try {tries} of {retries} n\\ {r.headers}\")\n",
    "    return print(f\"{url} has problems\")\n",
    "\n",
    "\n",
    "def calling(keyword, location, days, api_key, last_id, page_count):\n",
    "    \"\"\"Bundles methods and returns id html.\"\"\"\n",
    "    id_url = names.url_next(keyword, location, days, last_id, page_count)\n",
    "    r = scrape(3, api_key, id_url)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids\n",
    "\n",
    "\"\"\"Modules to retrieve and store Indeed Ids\"\"\"\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_ids(r):\n",
    "    \"\"\"convert search page response to list of ids.\"\"\"\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    td_soup = soup.find_all(\"h2\")\n",
    "    jk_list = []\n",
    "    for soup in td_soup:\n",
    "        aa = soup.a\n",
    "        try:\n",
    "            jk = aa.attrs[\"data-jk\"]\n",
    "            jk_list.append(jk)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    return jk_list\n",
    "\n",
    "\n",
    "def get_new_ids(r, file_name, old_ids):\n",
    "    # takes allIds and pageCount from previous loop if available\n",
    "    \"\"\"Returns a uniques set of ids consisting of old ids and this loop.\"\"\"\n",
    "    # loads saved file if first loop\n",
    "\n",
    "    if len(old_ids) == 0:\n",
    "        existing_ids_load = j_load(file_name)\n",
    "        existing_id_count = existing_ids_load[0]\n",
    "        existing_ids = existing_ids_load[1]\n",
    "        existing_id_dd = dd(existing_ids)\n",
    "        print(f\"starting id count is {existing_ids_load[0]}\")\n",
    "    else:\n",
    "        existing_id_dd = dd(old_ids)\n",
    "        existing_id_count = len(existing_id_dd)\n",
    "    try:\n",
    "        loop_id_list = get_ids(r)\n",
    "        all_unique = dd(existing_id_dd + loop_id_list)\n",
    "    except (NameError, TypeError, AttributeError):\n",
    "        all_unique = []\n",
    "    all_unique_count = len(all_unique)\n",
    "    loop_unique_count = all_unique_count - existing_id_count\n",
    "    print(\n",
    "        f\"\\n{dt.now()} page_unique_count: {loop_unique_count} currentCount: {all_unique_count}\"\n",
    "    )\n",
    "    return all_unique\n",
    "\n",
    "\n",
    "def zero_count(loop_count):\n",
    "    \"\"\"Returns false when 0 new ids returned twice in a row.\n",
    "    Used to end scraping\"\"\"\n",
    "    if len(loop_count) <= 2:\n",
    "        return True\n",
    "    if loop_count[-1] + loop_count[-2] > 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def just_ids(keyword=None, location=None, api_key=None, days=None, folder_path=None):\n",
    "    \"\"\"Assembles methods for IDs.  Save and count all.  Keyboard interupt causes end and save\"\"\"\n",
    "    current_id_list = dd([])\n",
    "    loop_count_list = []\n",
    "    page_count = 0\n",
    "    file_name = names.file_name_ids(folder_path, keyword, location)\n",
    "    # breaks loop when 2 consecutive loops return 0 new ids\n",
    "    try:\n",
    "        while zero_count(loop_count_list):\n",
    "            try:\n",
    "                last_id = current_id_list[-1]\n",
    "            except IndexError:\n",
    "                last_id = str()\n",
    "            try:\n",
    "                r = scrape.calling(\n",
    "                    keyword, location, days, api_key, last_id, page_count\n",
    "                )\n",
    "            except urllib3.exceptions.ReadTimeoutError as e:\n",
    "                print(f\"{e}\")\n",
    "                j_dump(file_name, current_id_list)\n",
    "            newest_ids = get_new_ids(r, file_name, current_id_list)\n",
    "            try:\n",
    "                loop_count = len(newest_ids) - len(current_id_list)\n",
    "            except TypeError:\n",
    "                loop_count = len(newest_ids)\n",
    "            loop_count_list.append(loop_count)\n",
    "            page_count += 1\n",
    "            for new in newest_ids:\n",
    "                current_id_list.append(new)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    j_dump(file_name, dd(current_id_list))\n",
    "    print(\n",
    "        f\"Finished!  New id count is {len(dd(current_id_list))} \\\n",
    "        Saved to: {file_name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snot\n",
    "\n",
    "\"\"\"Collection of python snowflake tools\"\"\"\n",
    "\n",
    "import os\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ctx = snowflake.connector.connect(\n",
    "    account=os.getenv(\"ACCOUNT\"),\n",
    "    user=os.getenv(\"SF_USER\"),\n",
    "    password=os.getenv(\"PASSWORD\"),\n",
    "    role=os.getenv(\"ROLE\"),\n",
    "    database=os.getenv(\"DATABASE\"),\n",
    "    warehouse=os.getenv(\"WAREHOUSE\"),\n",
    "    schema=os.getenv(\"SCHEMA\"),\n",
    ")\n",
    "\n",
    "\n",
    "def df_to_table(df=None, table=None) -> dict:\n",
    "    \"\"\"Appends df to snowflake table or creates new table\"\"\"\n",
    "    conn = snowflake.connector.connect(\n",
    "        account=os.getenv(\"ACCOUNT\"),\n",
    "        user=os.getenv(\"SF_USER\"),\n",
    "        password=os.getenv(\"PASSWORD\"),\n",
    "        role=os.getenv(\"ROLE\"),\n",
    "        database=os.getenv(\"DATABASE\"),\n",
    "        warehouse=os.getenv(\"WAREHOUSE\"),\n",
    "        schema=os.getenv(\"SCHEMA\"),\n",
    "    )\n",
    "    str_df = df.astype(str)\n",
    "    success, chunks, rows, snowflake_output = write_pandas(\n",
    "        conn=conn, df=str_df, table_name=table, auto_create_table=True\n",
    "    )\n",
    "    if success is True:\n",
    "        print(f\"Success!  {rows} rows added to {table} in {chunks} chunks\")\n",
    "    else:\n",
    "        print(f\"DID NOT WORK! {table} \\n {snowflake_output}\")\n",
    "\n",
    "\n",
    "def current_ids(table_name=None):\n",
    "    \"\"\"retrieves existing ids to avoid duplication\"\"\"\n",
    "\n",
    "    with snowflake.connector.connect(\n",
    "        account=os.getenv(\"ACCOUNT\"),\n",
    "        user=os.getenv(\"SF_USER\"),\n",
    "        password=os.getenv(\"PASSWORD\"),\n",
    "        role=os.getenv(\"ROLE\"),\n",
    "        database=os.getenv(\"DATABASE\"),\n",
    "        warehouse=os.getenv(\"WAREHOUSE\"),\n",
    "        schema=os.getenv(\"SCHEMA\"),\n",
    "    ) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(f'select \"job_id\" from {table_name}')\n",
    "            id_list = []\n",
    "            for col1 in cur:\n",
    "                idee = col1[0]\n",
    "                id_list.append(idee)\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Remote']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "location = os.getenv(\"LOCATION\").split()\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract\n",
    "\n",
    "\"\"\"Filter and extract job data based on ids\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "scraper_api = os.getenv(\"API_KEY\")\n",
    "\n",
    "\n",
    "def get_job_dict(id_tuple):\n",
    "    \"\"\"Makes api call and converts html response to soup,\n",
    "    extracts specific json, converts to dict return\"\"\"\n",
    "    keyword = id_tuple[0]\n",
    "    location = id_tuple[1]\n",
    "    id_number = id_tuple[2]\n",
    "    url = id_tuple[3]\n",
    "    r = scrape.scrape(3, scraper_api, url).text\n",
    "    try:\n",
    "        # create soup from r\n",
    "        soup = BeautifulSoup(r, \"html.parser\")\n",
    "        # find and extract json from soup\n",
    "        j_soup = soup.find(\"script\", type=\"application/ld+json\").text\n",
    "        # convert json to dict\n",
    "        d_soup = json.loads(j_soup)\n",
    "        # Append data from tuple\n",
    "        d_soup.update(\n",
    "            {\n",
    "                \"jobId\": id_number,\n",
    "                \"jobKeyword\": keyword,\n",
    "                \"jobSearchLocation\": location,\n",
    "            }\n",
    "        )\n",
    "        # print(f'{dt.now()}get_job_dict try')\n",
    "        return d_soup\n",
    "    except AttributeError:\n",
    "        # print(f'{dt.now()}get_job_dict ex')\n",
    "        return {\n",
    "            \"jobId\": id_number,\n",
    "            \"jobKeyword\": keyword,\n",
    "            \"jobSearchLocation\": location,\n",
    "        }\n",
    "\n",
    "\n",
    "# Ids ready\n",
    "def ids_warming(path, table_name):\n",
    "    \"\"\"Extract ids from json files.  Ouput list of tuples with keyword, location, id, URL\"\"\"\n",
    "    id_tuple_list = []\n",
    "    # get ids that have already run\n",
    "    df_ids = current_ids(table_name)\n",
    "    # list of id file names\n",
    "    ids = glob.glob(f\"{path}/indeedIds_*.json\")\n",
    "    # loop through file names extract kw and location\n",
    "    for d in ids:\n",
    "        id_path = d.split(\"/\")[-1]\n",
    "        id_name = id_path.split(\"_\")\n",
    "        kw = id_name[1]\n",
    "        loc = id_name[2].split(\".\")[0]\n",
    "        # open each id file\n",
    "        with open(d, \"r\", encoding=\"utf-8\") as f:\n",
    "            j_ids = json.load(f)[d]\n",
    "            # if id already in df skip\n",
    "            for j in j_ids:\n",
    "                if j in df_ids:\n",
    "                    pass\n",
    "                elif kw in (\"test\", \"write\"):\n",
    "                    pass\n",
    "                # not in df... create tup using (kw,loc,id) and append to list\n",
    "                else:\n",
    "                    url = f\"https://www.indeed.com/m/viewjob?jk={j}\"\n",
    "                    id_tup = (kw, loc, j, url)\n",
    "                    id_tuple_list.append(id_tup)\n",
    "    # return set of all unique unprocessed tuples\n",
    "    tuple_dd_list = dd(id_tuple_list)\n",
    "    print(f\"{len(tuple_dd_list)} unique job ids will be processed\")\n",
    "    return tuple_dd_list\n",
    "\n",
    "\n",
    "# This func does not work as stand alone\n",
    "def li_li_li_list(description):\n",
    "    \"\"\"Takes description dict and returns li values as list\"\"\"\n",
    "    # Resoup dict\n",
    "    d_soup = BeautifulSoup(description, \"html.parser\")\n",
    "    # get list of li tag text\n",
    "    lis = d_soup.find_all(\"li\")\n",
    "    tag_list = []\n",
    "    for l in lis:\n",
    "        # get just li text and put back in list\n",
    "        bullet = l.text\n",
    "        tag_list.append(bullet)\n",
    "    # print(f'{dt.now()}li_li_li_list')\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Design', 'data']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spac\n",
    "\n",
    "\"\"\"Spacy nlp functions\"\"\"\n",
    "\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "stopwords = list(nlp.Defaults.stop_words)\n",
    "\n",
    "\n",
    "def spacy_proper(doc):\n",
    "    \"\"\"Takes string as input and returns a list of Proper Nouns\"\"\"\n",
    "    pn_list = []\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == \"PROPN\":\n",
    "            pn_list.append(tok.text)\n",
    "        else:\n",
    "            pass\n",
    "    return pn_list\n",
    "\n",
    "\n",
    "def sentence_parse_proper(sentences):\n",
    "    \"\"\"Parses list of sentences and returns list of proper nouns\"\"\"\n",
    "    col_lists = []\n",
    "    try:\n",
    "        for sentence in sentences:\n",
    "            ss = sentence.strip()\n",
    "            doc = nlp(ss)\n",
    "            pn = spacy_proper(doc)\n",
    "            col_lists.append(pn)\n",
    "    except ValueError:\n",
    "        col_lists.append([])\n",
    "    return set(chain.from_iterable(col_lists))\n",
    "\n",
    "\n",
    "def pattern_lower(csv_file, column_name):\n",
    "    \"\"\"Formats column from csv file into patterns for matching\"\"\"\n",
    "    pattern_list = []\n",
    "    df = pd.read_csv(csv_file)\n",
    "    word_list = list(df[column_name])\n",
    "    clean_word_list = [str(t).lower().strip() for t in word_list if t is not np.nan]\n",
    "    split_list = [t.split() for t in clean_word_list]\n",
    "    for i in range(len(split_list)):\n",
    "        words = []\n",
    "        sentence = split_list[i]\n",
    "        for w in sentence:\n",
    "            pattern = dict(LOWER=str(w))\n",
    "            words.append(pattern)\n",
    "        pattern_list.append(words)\n",
    "    # print(f'{column_name} now contains {len(pattern_list)} keywords')\n",
    "    return pattern_list\n",
    "\n",
    "\n",
    "def data_word_match(sentence, csv_file, column_name):\n",
    "    \"\"\"Matches phrase patterns\"\"\"\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    word_patterns = pattern_lower(csv_file, column_name)\n",
    "    matcher.add(column_name, word_patterns, greedy=\"FIRST\")\n",
    "    doc = nlp(sentence)\n",
    "    matches = matcher(\n",
    "        doc\n",
    "    )  # [(15794293310264179361, 0, 1), (15794293310264179361, 7, 8)]\n",
    "    words = []\n",
    "    for match_id, start, end in matches:  ##match_id, not used\n",
    "        span = doc[start:end]\n",
    "        words.append(span.text)\n",
    "    return list(words)\n",
    "\n",
    "\n",
    "sentence = \"Design and build new ELT-based data models using SQL and dbt\"\n",
    "csv_file = \"Data/snow_words.csv\"\n",
    "column_name = \"Data_Skills\"\n",
    "data_word_match(sentence, csv_file, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use lemmatization to match base words instead of exact # [fixme]\n",
    "\n",
    "\n",
    "def sentence_parse_data_words(sentences, csv_file, column_name):\n",
    "    \"\"\"Takes string as input and returns a list of Proper Nouns\"\"\"\n",
    "    words_lists = []\n",
    "    try:\n",
    "        for sentence in sentences:\n",
    "            words = data_word_match(sentence, csv_file, column_name)\n",
    "            words_lists.append(words)\n",
    "    except ValueError:\n",
    "        words_lists.append([])\n",
    "    return set(chain.from_iterable(words_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch-dict\n",
    "#\n",
    "# \"\"\"Handles errors and assembles job dict\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def check_and_extract(full_dict):\n",
    "    \"\"\"This function is designed to catch errors in the dictionary.\n",
    "    Function li_li_li_list extracts list of values.  Dict with data and lables returned\n",
    "    \"\"\"\n",
    "    try:\n",
    "        company = full_dict[\"hiringOrganization\"][\"name\"]\n",
    "    except KeyError:\n",
    "        company = \"Unavailable\"\n",
    "    except TypeError:\n",
    "        company = \"Unavailable\"\n",
    "    try:\n",
    "        title = full_dict[\"title\"]\n",
    "    except TypeError:\n",
    "        title = \"Unavailable\"\n",
    "    except KeyError:\n",
    "        title = \"Unavailable\"\n",
    "    try:\n",
    "        base_salary_low = full_dict[\"baseSalary\"][\"value\"][\"minValue\"]\n",
    "    except KeyError:\n",
    "        base_salary_low = \"Unavailable\"\n",
    "    except TypeError:\n",
    "        base_salary_low = \"Unavailable\"\n",
    "    try:\n",
    "        base_salary_high = full_dict[\"baseSalary\"][\"value\"][\"maxValue\"]\n",
    "    except KeyError:\n",
    "        base_salary_high = \"Unavailable\"\n",
    "    except TypeError:\n",
    "        base_salary_high = \"Unavailable\"\n",
    "    try:\n",
    "        salary_period = full_dict[\"baseSalary\"][\"value\"][\"unitText\"]\n",
    "    except KeyError:\n",
    "        salary_period = \"Unavailable\"\n",
    "    except TypeError:\n",
    "        salary_period = \"Unavailable\"\n",
    "    try:\n",
    "        date_posted = full_dict[\"datePosted\"]\n",
    "    except KeyError:\n",
    "        date_posted = \"Unavailable\"\n",
    "    except TypeError:\n",
    "        date_posted = \"Unavailable\"\n",
    "    try:\n",
    "        date_expires = full_dict[\"date_expires\"]\n",
    "    except TypeError:\n",
    "        date_expires = \"Unavailable\"\n",
    "    except KeyError:\n",
    "        date_expires = \"Unavailable\"\n",
    "    try:\n",
    "        employment_type = full_dict[\"employmentType\"]\n",
    "    except KeyError:\n",
    "        employment_type = \"Unavailable\"\n",
    "    except TypeError:\n",
    "        employment_type = \"Unavailable\"\n",
    "    try:\n",
    "        location = full_dict[\"jobLocation\"][\"address\"][\"addressLocality\"]\n",
    "    except KeyError:\n",
    "        location = \"Unavailable\"\n",
    "    except TypeError:\n",
    "        location = \"Unavailable\"\n",
    "    try:\n",
    "        description_raw = full_dict[\"description\"]\n",
    "    except KeyError:\n",
    "        description_raw = \"Unavailable\"\n",
    "    except TypeError:\n",
    "        description_raw = \"Unavailable\"\n",
    "    try:\n",
    "        description_list = li_li_li_list(description_raw)\n",
    "    except KeyError:\n",
    "        description_list = \"Unavailable\"\n",
    "    except TypeError:\n",
    "        description_list = \"Unavailable\"\n",
    "    skills = sentence_parse_data_words(\n",
    "        description_list, \"Data/snow_words.csv\", \"Data_Skills\"\n",
    "    )\n",
    "    data_skills = [x.lower() for x in skills]\n",
    "    technology = sentence_parse_data_words(\n",
    "        description_list, \"Data/snow_words.csv\", \"Data_Technology\"\n",
    "    )\n",
    "    data_technology = [x.lower() for x in technology]\n",
    "    propers = sentence_parse_proper(description_list)\n",
    "    proper_nouns = [\n",
    "        x.lower()\n",
    "        for x in propers\n",
    "        if x.lower() not in data_skills and x.lower() not in data_technology\n",
    "    ]\n",
    "    job_id = full_dict[\"jobId\"]\n",
    "    keyword = full_dict[\"jobKeyword\"]\n",
    "    search_location = full_dict[\"jobSearchLocation\"]\n",
    "    print(\n",
    "        f\"{job_id}  {keyword}   {search_location}  {company}  {title}  {date_posted}\\\n",
    "          {employment_type}  {location}  \\n  {proper_nouns} \\n {data_skills} \\n {data_technology}\"\n",
    "    )\n",
    "    job_dict = {\n",
    "        \"job_id\": job_id,\n",
    "        \"entered\": time.strftime(\"%D %T\"),\n",
    "        \"search_keyword\": keyword,\n",
    "        \"search_location\": search_location,\n",
    "        \"job_company\": company,\n",
    "        \"job_title\": title,\n",
    "        \"job_date_posted\": date_posted,\n",
    "        \"job_date_expires\": date_expires,\n",
    "        \"pay_low\": base_salary_low,\n",
    "        \"pay_high\": base_salary_high,\n",
    "        \"pay_period\": salary_period,\n",
    "        \"job_type\": employment_type,\n",
    "        \"job_location\": location,\n",
    "        \"description_sentences\": description_list,\n",
    "        \"proper_nouns\": proper_nouns,\n",
    "        \"data_skills\": data_skills,\n",
    "        \"data_technology\": data_technology,\n",
    "    }\n",
    "    return job_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words\n",
    "\n",
    "\"\"\"Combines and executes word functions to create final output\"\"\"\n",
    "\n",
    "import traceback\n",
    "import time\n",
    "import pandas as pd\n",
    "import extract\n",
    "import catch_dict\n",
    "import snot\n",
    "\n",
    "now = int(time.time() * 1000000)\n",
    "\n",
    "\n",
    "def come_together(path, table_name):\n",
    "    \"\"\"runs scraped ids and returns list df\"\"\"\n",
    "    dict_list = []\n",
    "    tup_list = extract.ids_warming(\n",
    "        path, table_name\n",
    "    )  # returns list of id tuples  ->list\n",
    "    er_count = 0\n",
    "    try:\n",
    "        for tup in tup_list:\n",
    "            try:\n",
    "                job_dict = extract.get_job_dict(\n",
    "                    tup\n",
    "                )  # takes one id tuple and outputs dict with data -> dict\n",
    "                new_job_dict = catch_dict.check_and_extract(\n",
    "                    job_dict\n",
    "                )  #  (description_list:[]) -> dict\n",
    "                # should only run ids not already in table\n",
    "                dict_list.append(new_job_dict)\n",
    "                print(f\"{len(dict_list)} jobs out of {len(tup_list)} processed\")\n",
    "            except TimeoutError:\n",
    "                traceback.print_exc()\n",
    "                er_count += 1\n",
    "                er_retry = er_count * 60\n",
    "                print(f\"Error has occured.  Will retry in {er_retry}\")\n",
    "        df_new = pd.DataFrame(dict_list)\n",
    "        snot.df_to_table(df_new, table_name)\n",
    "        return df_new\n",
    "    except KeyboardInterrupt:\n",
    "        df_new = pd.DataFrame(dict_list)\n",
    "        snot.df_to_table(df_new, table_name)\n",
    "        return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxxxxx'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import snowflake.connector\n",
    "\n",
    "\n",
    "def x_obscure(string):\n",
    "    letters = []\n",
    "    for s in string:\n",
    "        letters.append(\"x\")\n",
    "    xx = \"\".join(letters)\n",
    "    return xx\n",
    "\n",
    "\n",
    "x_obscure(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snowflake_column(\n",
    "    sf_column_name,\n",
    "    sf_table_name,\n",
    "    sf_account,\n",
    "    sf_user,\n",
    "    sf_password,\n",
    "    sf_database,\n",
    "    sf_warehouse,\n",
    "    sf_schema,\n",
    "):\n",
    "    \"\"\"gets one column from snowflake table\"\"\"\n",
    "    with snowflake.connector.connect(\n",
    "        user=sf_user,\n",
    "        password=sf_password,\n",
    "        account=sf_account,\n",
    "        warehouse=sf_warehouse,\n",
    "        database=sf_database,\n",
    "        schema=sf_schema,\n",
    "    ) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(f\"select {sf_column_name} from {sf_table_name}\")\n",
    "            c_list = []\n",
    "            for col1 in cur:\n",
    "                c = col1[0]\n",
    "                c_list.append(c)\n",
    "    return c_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORD = os.getenv(\"KEYWORD\")\n",
    "LOCATION = os.getenv(\"LOCATION\")  # list\n",
    "API_KEY = os.getenv(\"API_KEY\")  # one variable\n",
    "DAYS = os.getenv(\"DAYS\")  # one variable\n",
    "FOLDER = os.getenv(\"FOLDER\")  # one variable\n",
    "TABLE_NAME = os.getenv(\"TABLE_NAME\")  # one variable\n",
    "SF_USER = os.getenv(\"SF_USER\")\n",
    "PASSWORD = os.getenv(\"PASSWORD\")\n",
    "ACCOUNT = os.getenv(\"ACCOUNT\")\n",
    "ROLE = os.getenv(\"ROLE\")\n",
    "WAREHOUSE = os.getenv(\"WAREHOUSE\")\n",
    "DATABASE = os.getenv(\"DATABASE\")\n",
    "SCHEMA = os.getenv(\"SCHEMA\")\n",
    "MATCH_WORDS = os.getenv(\"MATCH_WORDS\")\n",
    "SF_SKILL_WORD_TABLE = os.getenv(\"SF_SKILL_WORD_TABLE\")\n",
    "SF_TECHNOLOGY_WORD_TABLE = os.getenv(\"SF_TECHNOLOGY_WORD_TABLE\")\n",
    "SF_MATCH_COLUMN_SKILLS = os.getenv(\"SF_MATCH_COLUMN_SKILLS\")\n",
    "SF_MATCH_COLUMN_TECHNOLOGY = os.getenv(\"SF_MATCH_COLUMN_TECHNOLOGY\")\n",
    "DATA_FOLDER_PATH = os.getenv(\"DATA_FOLDER_PATH\")\n",
    "BACKUP_FOLDER = os.getenv(\"BACKUP_FOLDER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.net driver',\n",
       " 'ab initio',\n",
       " 'adobe  campaign',\n",
       " 'adobe  experience  manager',\n",
       " 'adobe  journey  optimizer',\n",
       " 'aginity pro / aginity team',\n",
       " 'alation',\n",
       " 'altr',\n",
       " 'amazon sagemaker',\n",
       " 'asp net',\n",
       " 'astrato',\n",
       " 'aws quicksight',\n",
       " 'azure',\n",
       " 'cdata',\n",
       " 'comforte',\n",
       " 'datadog',\n",
       " 'datarobot',\n",
       " 'ora2pg',\n",
       " 'agile data engine',\n",
       " 'essbase',\n",
       " 'intercom',\n",
       " 'active takeoff and plan grid',\n",
       " 'dvsum',\n",
       " 'crystal  reports',\n",
       " 'google cloud',\n",
       " 'protegrity',\n",
       " 'carisk  portal',\n",
       " 'c++',\n",
       " 'immuta',\n",
       " 'hyper  v',\n",
       " 'outreach',\n",
       " 'amazon',\n",
       " 'data_technology',\n",
       " 'hevo data',\n",
       " 'alteryx',\n",
       " 'datastage',\n",
       " 'hvr',\n",
       " 'bigeye',\n",
       " 'chartio',\n",
       " 'informatica powercenter',\n",
       " 'excel',\n",
       " 'knoema',\n",
       " 'python',\n",
       " 'salesforce',\n",
       " 'carisk  waitlist',\n",
       " 'hightouch',\n",
       " 'oracle data enterprise quality',\n",
       " 'precog',\n",
       " 'big squid',\n",
       " 'cognos',\n",
       " 'gcp',\n",
       " 'google cloud dataflow',\n",
       " 'devsecops',\n",
       " 'nexla',\n",
       " 'atlan',\n",
       " 'aurora',\n",
       " 'sas',\n",
       " 'spotfire',\n",
       " 'atscale',\n",
       " 'oracle',\n",
       " 'google',\n",
       " 'metabase',\n",
       " 'google cloud data fusion',\n",
       " 'on-screen take-off',\n",
       " 'active takeoff',\n",
       " 'adobe  target',\n",
       " 'etleap',\n",
       " 'microstrategy',\n",
       " 'snowflake time travel',\n",
       " 'anomalo',\n",
       " 'satori',\n",
       " 'collibra',\n",
       " 'sap businessobjects',\n",
       " 'odbc client driver',\n",
       " 'ec2',\n",
       " 'sigma computing',\n",
       " 'avro',\n",
       " 'ovaledge',\n",
       " 'h2o.ai',\n",
       " 'sqlalchemy',\n",
       " 'bash',\n",
       " 'cmpro',\n",
       " 'datavirtuality',\n",
       " 'fivetran',\n",
       " 'adobe',\n",
       " 'hive',\n",
       " 'dataops.live',\n",
       " 'cad',\n",
       " 'ibm cognos analytics',\n",
       " 'qubole',\n",
       " 'hunters',\n",
       " 'jsonar',\n",
       " 'netsuite',\n",
       " 'seekwell',\n",
       " 'aginity',\n",
       " 'boostkpi',\n",
       " 'apple',\n",
       " 'azure data factory',\n",
       " 'alteryx designer cloud',\n",
       " 'keboola',\n",
       " 'fortanix',\n",
       " 'rds',\n",
       " 'sqldbm',\n",
       " 'glue',\n",
       " 'impala',\n",
       " 'oracle application development framework (adf)',\n",
       " 'privacera',\n",
       " 'sigma',\n",
       " 'azure machine learning',\n",
       " 'amazon data firehose',\n",
       " 'r',\n",
       " 'scikit',\n",
       " 'sns',\n",
       " 'orc',\n",
       " 'scala',\n",
       " 'dbt labs',\n",
       " 'microsoft  hyper  v',\n",
       " 'okera',\n",
       " 'domino',\n",
       " 'sap',\n",
       " 'oracle data integrator',\n",
       " 'dbeaver',\n",
       " 'etlworks',\n",
       " 'hashicorp vault',\n",
       " 'doors',\n",
       " 'sap business intelligence',\n",
       " 'businessworks',\n",
       " 'census',\n",
       " 'matillion data loader',\n",
       " 'pentaho business analytics',\n",
       " 'ci poly',\n",
       " 'go',\n",
       " 'sap data services',\n",
       " 'bigid',\n",
       " 'activematrix',\n",
       " 'chargify',\n",
       " 'spark connector',\n",
       " 'sql workbench',\n",
       " 'pyramid',\n",
       " 'ibm',\n",
       " 'matillion etl',\n",
       " 'adobe campaign',\n",
       " 'baffle',\n",
       " 'jdbc client driver',\n",
       " 'thoughtspot',\n",
       " 'hubspot',\n",
       " 'oracle jdeveloper',\n",
       " 'qlik sense',\n",
       " 'google’s ai',\n",
       " 'oracle analytics server (oas)',\n",
       " 'pytorch',\n",
       " 'rdma',\n",
       " 'sisense',\n",
       " 'skyvia',\n",
       " 'snowflake scripting',\n",
       " 'qlik replicate',\n",
       " 'sqs',\n",
       " 'striim',\n",
       " 'tableau prep',\n",
       " 'tibco spotfire',\n",
       " 'carto',\n",
       " 'microsoft .net',\n",
       " 'php pdo',\n",
       " 'stitch',\n",
       " 'dataguise',\n",
       " 'sharepoint',\n",
       " 'object-oriented',\n",
       " 'kafka',\n",
       " 'tealium',\n",
       " 'visual  studio',\n",
       " 'snowpipe',\n",
       " 'devart odbc',\n",
       " 'acryl data',\n",
       " 'devart ssis',\n",
       " 'aws',\n",
       " 'microsoft power bi',\n",
       " 'boomi',\n",
       " 'airflow',\n",
       " 'diyotta',\n",
       " 'pentaho',\n",
       " 'cyberres voltage',\n",
       " 'erwin',\n",
       " 'clearwell',\n",
       " 'hevo',\n",
       " 'solita',\n",
       " 'topvue',\n",
       " 'crystal',\n",
       " 'devart',\n",
       " 'elasticsearch',\n",
       " 'onetrust',\n",
       " 'informatica data governance and compliance',\n",
       " 'k',\n",
       " 'sagemaker',\n",
       " 'streamsets',\n",
       " 'php pdo driver',\n",
       " 'graybar',\n",
       " 'datameer',\n",
       " 'snowpark container service',\n",
       " 'word',\n",
       " 'carisk',\n",
       " 'cdata software',\n",
       " 'go snowflake driver',\n",
       " 'lacework',\n",
       " 'airbyte',\n",
       " 'solace',\n",
       " 'snowflake-managed keys',\n",
       " 'ascend.io',\n",
       " 'cyberres',\n",
       " 'object-oriented programming languages',\n",
       " 'jdbc',\n",
       " 'docker',\n",
       " 'coalesce',\n",
       " 'object-oriented programming',\n",
       " 'secupi',\n",
       " 'node.js',\n",
       " 'google data studio',\n",
       " 'parquet',\n",
       " 'hashicorp',\n",
       " 'microsoft  dynamics',\n",
       " 'pl  sql',\n",
       " 'denodo',\n",
       " 'microsoft ai',\n",
       " 'spring labs',\n",
       " 'redshift',\n",
       " 'hadoop',\n",
       " 'tableau crm',\n",
       " 'hackolade',\n",
       " 'node.js driver',\n",
       " 'odbc',\n",
       " 'trifacta',\n",
       " 'dbt',\n",
       " 'domo',\n",
       " 'apache nifi',\n",
       " 'jasper',\n",
       " 'data factory',\n",
       " 'dataiku',\n",
       " 'hyperion',\n",
       " 'plsql',\n",
       " 'hex',\n",
       " 'informatica cloud',\n",
       " 'pl/pgsql',\n",
       " 'qlik',\n",
       " 's3',\n",
       " 'informatica data loader',\n",
       " 'matillion',\n",
       " 'skyflow',\n",
       " 'sled',\n",
       " 'socs',\n",
       " 'power bi',\n",
       " 'jmeter',\n",
       " 'data.world',\n",
       " 'ca erwin',\n",
       " 'looker',\n",
       " 'mapreduce',\n",
       " 'jira',\n",
       " 'monte carlo',\n",
       " 'segment',\n",
       " 'macheye',\n",
       " 'statsig',\n",
       " 't  sql',\n",
       " 'tensorflow',\n",
       " 'trustlogix',\n",
       " 'workato',\n",
       " 'xml',\n",
       " 'jenkins',\n",
       " 'oracle public sector planning and budgeting',\n",
       " 'pih/pii',\n",
       " 'pl',\n",
       " 'python connector',\n",
       " 'tellius',\n",
       " 'thales',\n",
       " 'toad',\n",
       " None,\n",
       " 'tibco activematrix businessworks',\n",
       " 'informatica',\n",
       " 'wherescape',\n",
       " 'rivery',\n",
       " 'select star',\n",
       " 'snowplow',\n",
       " 'spark',\n",
       " 'tamr',\n",
       " 'json',\n",
       " 'visual  basic',\n",
       " 'pentaho data integration',\n",
       " 'oracle sql',\n",
       " 'mpi',\n",
       " 'xcelsius',\n",
       " 'python package',\n",
       " 'pii',\n",
       " 'vmware',\n",
       " 'confluence',\n",
       " 'powerpoint',\n",
       " 'artie',\n",
       " 'mode',\n",
       " 'oracle analytics',\n",
       " 'outlook',\n",
       " 'databricks',\n",
       " 'comptia security + ce',\n",
       " 'postgres',\n",
       " 'splunk',\n",
       " 'quicksight',\n",
       " 'supermetrics',\n",
       " 'tableau',\n",
       " 'talend',\n",
       " 'tmmdata',\n",
       " 'visio',\n",
       " 'webi',\n",
       " 'zepl',\n",
       " 'pyramid analytics',\n",
       " 'mlops',\n",
       " 'oracle business intelligence enterprise edition',\n",
       " 'dynamics',\n",
       " 'snaplogic',\n",
       " 'zapier',\n",
       " 'powercenter',\n",
       " 'sqlnavigator',\n",
       " 'ibm datastage',\n",
       " 'monday.com',\n",
       " 'tibco',\n",
       " 'python sql toolkit']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "technology_word_list = get_snowflake_column(\n",
    "    SF_MATCH_COLUMN_TECHNOLOGY,\n",
    "    SF_TECHNOLOGY_WORD_TABLE,\n",
    "    ACCOUNT,\n",
    "    SF_USER,\n",
    "    PASSWORD,\n",
    "    DATABASE,\n",
    "    WAREHOUSE,\n",
    "    SCHEMA,\n",
    ")\n",
    "\n",
    "technology_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(e)lt orchestration',\n",
       " 'accurately',\n",
       " 'acp',\n",
       " 'acquisition',\n",
       " 'administer',\n",
       " 'administration',\n",
       " 'aggregating',\n",
       " 'algorithms',\n",
       " 'analytics',\n",
       " 'analytics metasdata and metrics',\n",
       " 'anomalies',\n",
       " 'anomaly detection',\n",
       " 'anticipate',\n",
       " 'attrition',\n",
       " 'auditing',\n",
       " 'audits',\n",
       " 'automated data lineage catalog',\n",
       " 'biomedical',\n",
       " 'biometrics',\n",
       " 'biostatistics',\n",
       " 'business analytics suite',\n",
       " 'cleanup',\n",
       " 'competitive',\n",
       " 'concordance',\n",
       " 'customer data',\n",
       " 'exploration',\n",
       " 'apache',\n",
       " 'auditing data',\n",
       " 'cdisc',\n",
       " 'keras',\n",
       " 'apis',\n",
       " 'automated data lineage',\n",
       " 'construction',\n",
       " 'consulting',\n",
       " 'data activation platform',\n",
       " 'data quality monitoring',\n",
       " 'data quality monitoring and anomaly detection',\n",
       " 'data virtualization',\n",
       " 'erp',\n",
       " 'dhs',\n",
       " 'compatibility',\n",
       " 'econometric',\n",
       " 'anomaly',\n",
       " 'cibmtr',\n",
       " 'data loader',\n",
       " 'certified business intelligence professional',\n",
       " 'ci/cd automation',\n",
       " 'biology',\n",
       " 'cardiovascular',\n",
       " 'chemistry',\n",
       " 'computing graphics environment',\n",
       " 'data virtualization and federation platform',\n",
       " 'embryology',\n",
       " 'csm',\n",
       " 'digital properties',\n",
       " 'dashboards',\n",
       " 'compelling data',\n",
       " 'igniting',\n",
       " 'certified  health  data  analyst',\n",
       " 'fraud trends',\n",
       " 'meet deadlines',\n",
       " 'identity',\n",
       " 'informatics',\n",
       " 'iot',\n",
       " 'assess',\n",
       " 'assessing impacts',\n",
       " 'broader systems',\n",
       " 'completeness',\n",
       " 'computer science',\n",
       " 'configuration',\n",
       " 'consumable',\n",
       " 'b2b',\n",
       " 'collaborative',\n",
       " 'comptia security+ certification',\n",
       " 'automation',\n",
       " 'cloud-native',\n",
       " 'biopharmaceutical',\n",
       " 'compensation',\n",
       " 'irb',\n",
       " 'evangelize',\n",
       " 'desktop analytics',\n",
       " 'annotated',\n",
       " 'b2c',\n",
       " 'analytics metadata and metrics',\n",
       " 'business intelligence tools',\n",
       " 'classification',\n",
       " 'cloud-based',\n",
       " 'data clean-up',\n",
       " 'data loading and migration',\n",
       " 'deadlines',\n",
       " 'big data',\n",
       " 'architecture',\n",
       " 'ci  sox',\n",
       " 'ci/cd',\n",
       " 'data engineering',\n",
       " 'bigdata',\n",
       " 'cloud-based data loader',\n",
       " 'complex sql',\n",
       " 'cybersecurity',\n",
       " 'data  privacy  protection  dpp',\n",
       " 'data governance',\n",
       " 'de&i',\n",
       " 'decision  science',\n",
       " 'dvd',\n",
       " 'enforcement',\n",
       " 'capacity  planning',\n",
       " 'change  control',\n",
       " 'cross-functionally',\n",
       " 'curation',\n",
       " 'data access management',\n",
       " 'detail',\n",
       " 'enterprise data',\n",
       " 'evaluate',\n",
       " 'extract',\n",
       " 'gathering',\n",
       " 'interactive',\n",
       " 'manager',\n",
       " 'math',\n",
       " 'measurement',\n",
       " 'performance data ',\n",
       " 'pmp',\n",
       " 'programming',\n",
       " 'crf',\n",
       " 'datasecops',\n",
       " 'development',\n",
       " 'express concepts',\n",
       " 'data streams',\n",
       " 'data visualization',\n",
       " 'decision trees',\n",
       " 'distributed streaming',\n",
       " 'customer  journey',\n",
       " 'cloud computing',\n",
       " 'cryptographic',\n",
       " 'data strategy',\n",
       " 'electronic  stored  information  esi',\n",
       " 'estimating',\n",
       " 'continuous data integration',\n",
       " 'de&i ',\n",
       " 'diplomacy',\n",
       " 'efficiently',\n",
       " 'gitlab',\n",
       " 'insights',\n",
       " 'interpret',\n",
       " 'marketing campaign management',\n",
       " 'promote',\n",
       " 'code  review',\n",
       " 'coordination',\n",
       " 'demonstrable',\n",
       " 'data science',\n",
       " 'top  secret',\n",
       " 'cipm',\n",
       " 'cross-platform',\n",
       " 'fci',\n",
       " 'federation',\n",
       " 'flexible',\n",
       " 'front-line ',\n",
       " 'distributed application',\n",
       " 'full-featured cloud-based',\n",
       " 'github',\n",
       " 'hcd',\n",
       " 'healthcare',\n",
       " 'hnhu',\n",
       " 'hr domain',\n",
       " 'image and video processing',\n",
       " 'institute of configuration management',\n",
       " 'j6 policy',\n",
       " 'java',\n",
       " 'jupyter',\n",
       " 'm',\n",
       " 'navwar',\n",
       " 'neural network',\n",
       " 'open database connectivity',\n",
       " 'optimizing code',\n",
       " 'policy management',\n",
       " 'power  performance  and area (ppa)',\n",
       " 'predictive',\n",
       " 'preprocessing',\n",
       " 'programmatic interfaces',\n",
       " 'quality',\n",
       " 'stored procedures',\n",
       " 'travel',\n",
       " 'functionalities',\n",
       " 'unstructured data',\n",
       " 'public cloud technologies',\n",
       " 'testing',\n",
       " 'java database connectivity',\n",
       " 'task goals',\n",
       " 'detection',\n",
       " 'corrections',\n",
       " 'cd',\n",
       " 'secrets',\n",
       " 'action plans',\n",
       " 'client interviews',\n",
       " 'data',\n",
       " 'enhance',\n",
       " 'enterprise data replication',\n",
       " 'data structures',\n",
       " 'environment',\n",
       " 'analytic cluster',\n",
       " 'analytics platform',\n",
       " 'balancing',\n",
       " 'enterprise integration',\n",
       " 'certifications',\n",
       " 'economics',\n",
       " 'external tokenization',\n",
       " 'corrective  actions  plans  caps',\n",
       " 'google sheets',\n",
       " 'integration',\n",
       " 'jde',\n",
       " 'junior',\n",
       " 'sirts',\n",
       " 'case  manage',\n",
       " 'data entry',\n",
       " 'dimensional models',\n",
       " 'distributed streaming platform',\n",
       " 'e2e',\n",
       " 'fine-tune',\n",
       " 'full-featured',\n",
       " 'cism',\n",
       " 'co-design',\n",
       " 'data lineage',\n",
       " 'datastudio',\n",
       " 'dcap',\n",
       " 'detail-oriented',\n",
       " 'ediscovery',\n",
       " 'enterprise-level',\n",
       " 'deep neural networks ',\n",
       " 'best practices',\n",
       " 'bi',\n",
       " 'compiling',\n",
       " 'data observability',\n",
       " 'campaign',\n",
       " 'cataloging',\n",
       " 'cipt',\n",
       " 'competencies',\n",
       " 'data_skills',\n",
       " 'developing',\n",
       " 'dynamic',\n",
       " 'facilitate',\n",
       " 'challenges',\n",
       " 'itbm',\n",
       " 'itsm',\n",
       " 'kimball’s',\n",
       " 'metrics',\n",
       " 'mis',\n",
       " 'ml',\n",
       " 'multiple task',\n",
       " 'calculating',\n",
       " 'complex business problems',\n",
       " 'concise communication',\n",
       " 'concurrent',\n",
       " 'continuous',\n",
       " 'cross-functional',\n",
       " 'cutting-edge',\n",
       " 'data loading',\n",
       " 'dsur',\n",
       " 'ehr',\n",
       " 'engineering',\n",
       " 'familiarity',\n",
       " 'framework',\n",
       " 'full funnel analytics',\n",
       " 'cipp',\n",
       " 'compliance',\n",
       " 'cultivate',\n",
       " 'curiosity',\n",
       " 'elicit',\n",
       " 'estimates',\n",
       " 'execute',\n",
       " 'd3.js',\n",
       " 'explain',\n",
       " 'programmatic',\n",
       " 'ci/cd orchestration environment',\n",
       " 'critical thinking',\n",
       " 'defense acquisition university configuration management',\n",
       " 'changing priorities',\n",
       " 'connectivity',\n",
       " 'enterprise etl',\n",
       " 'federal  agencies',\n",
       " 'foster',\n",
       " 'functions',\n",
       " 'data exploration',\n",
       " 'dataops',\n",
       " 'documentation',\n",
       " 'evolve',\n",
       " 'frameworks',\n",
       " 'einstein analytics',\n",
       " 'google analytic',\n",
       " 'guidelines',\n",
       " 'harness',\n",
       " 'hcm',\n",
       " 'human capital management (hcm)',\n",
       " 'identifiable  information',\n",
       " 'identify the inefficiencies',\n",
       " 'immunotherapy',\n",
       " 'initiative',\n",
       " 'intelligence',\n",
       " 'interactive data visualization',\n",
       " 'intricacies',\n",
       " 'itom',\n",
       " 'kernel.org',\n",
       " 'leadership',\n",
       " 'leverage data',\n",
       " 'machine learning ',\n",
       " 'machine learning platform',\n",
       " 'marketing',\n",
       " 'measurable',\n",
       " 'mentor',\n",
       " 'microsoft  teams',\n",
       " 'monitor',\n",
       " 'multiple projects',\n",
       " 'nlp',\n",
       " 'non-technical team members',\n",
       " 'nvidia',\n",
       " 'observability',\n",
       " 'ocr',\n",
       " 'on-premises',\n",
       " 'on-premises enterprise',\n",
       " 'optimizing',\n",
       " 'organizational',\n",
       " 'organizational objectives ',\n",
       " 'performing',\n",
       " 'photocopy',\n",
       " 'policies',\n",
       " 'potential',\n",
       " 'predictive analytics',\n",
       " 'prep',\n",
       " 'preparation',\n",
       " 'preprocess',\n",
       " 'problem-solving',\n",
       " 'programming language',\n",
       " 'regulations',\n",
       " 'reinforcement  learning',\n",
       " 'research',\n",
       " 'response  team',\n",
       " 'reviewing',\n",
       " 'schema',\n",
       " 'sdlc',\n",
       " 'security',\n",
       " 'self-service',\n",
       " 'senior',\n",
       " 'senior client',\n",
       " 'sensitive data',\n",
       " 'serverless',\n",
       " 'serverless data',\n",
       " 'serverless data integration',\n",
       " 'simulation',\n",
       " 'sql development',\n",
       " 'sql development & management',\n",
       " 'statistical computing',\n",
       " 'techniques',\n",
       " 'transformation',\n",
       " 'ts/sci',\n",
       " 'understand',\n",
       " 'validated',\n",
       " 'vast datasets',\n",
       " 'visual schema design',\n",
       " 'warehouse',\n",
       " 'web-based-document',\n",
       " 'willingness',\n",
       " 'machine-learning',\n",
       " \"master's\",\n",
       " 'methodologies',\n",
       " 'public  trust',\n",
       " 'reusable',\n",
       " 'scripting-based languages',\n",
       " 'self-service or enterprise',\n",
       " 'ssis',\n",
       " 'stakeholders',\n",
       " 'structured',\n",
       " 'sw/hw',\n",
       " 'unit testing',\n",
       " 'verbally',\n",
       " 'warehousing',\n",
       " 'professional',\n",
       " 'data activation',\n",
       " 'debugging',\n",
       " 'distribution',\n",
       " 'human  capital  management  hcm',\n",
       " 'improve data quality',\n",
       " 'independently',\n",
       " 'intel',\n",
       " 'learning',\n",
       " 'omni-channel analytics',\n",
       " 'on-premise',\n",
       " 'quality monitoring',\n",
       " 'ready',\n",
       " 'refining',\n",
       " 'robot data capture',\n",
       " 'self-starter',\n",
       " 'tokenization',\n",
       " 'user  acceptance',\n",
       " 'views',\n",
       " 'visual interfaces',\n",
       " 'interactive data',\n",
       " 'machine learning',\n",
       " 'productivity',\n",
       " 'recommendations',\n",
       " 'reusable and sharable sql',\n",
       " 'rules-based',\n",
       " 'tune',\n",
       " 'synthesizing',\n",
       " 'open source',\n",
       " 'statistical techniques',\n",
       " 'troubleshoots',\n",
       " 'sme',\n",
       " 'hardware',\n",
       " 'legacy',\n",
       " 'llm',\n",
       " 'mpa',\n",
       " 'projection',\n",
       " 'qa',\n",
       " 'reconcile',\n",
       " 'performance tuning',\n",
       " 'relationships',\n",
       " 'reusable and sharable',\n",
       " 'enterprise-level ssis',\n",
       " 'feature selection',\n",
       " 'generative ai',\n",
       " 'linear regression',\n",
       " 'object detection',\n",
       " 'pandas',\n",
       " 'query tool',\n",
       " 'visualization',\n",
       " 'people analytics',\n",
       " 'store',\n",
       " 'technical ambiguity',\n",
       " 'responsibility',\n",
       " 'scaling',\n",
       " 'scrum',\n",
       " 'fte',\n",
       " 'journeyman',\n",
       " 'tokenization solutions',\n",
       " 'unified analytics platform',\n",
       " 'trends',\n",
       " 'data pipelines',\n",
       " 'agile',\n",
       " 'datahub platform',\n",
       " 'gather',\n",
       " 'deploying',\n",
       " 'grafana',\n",
       " 'omni-channel',\n",
       " 'operationalize',\n",
       " 'backlog',\n",
       " 'discovery',\n",
       " 'dimension reduction',\n",
       " 'fraud',\n",
       " 'data flow',\n",
       " 'data pipeline',\n",
       " 'entrepreneurial',\n",
       " 'historical',\n",
       " 'infrastructure',\n",
       " 'interpersonal',\n",
       " 'learn',\n",
       " 'linux',\n",
       " 'metadata management',\n",
       " 'monitoring',\n",
       " 'operational  excellence',\n",
       " 'pdf',\n",
       " 'performance',\n",
       " 'personalization',\n",
       " 'present',\n",
       " 'problem solving',\n",
       " 'query',\n",
       " 're-usable development',\n",
       " 'replication',\n",
       " 'rules',\n",
       " 'schema design',\n",
       " 'sox',\n",
       " 'table indexes',\n",
       " 'tool chains',\n",
       " 'toolkit',\n",
       " 'unified stream and batch',\n",
       " 'multi-platform',\n",
       " 'orchestration',\n",
       " 'pipelines',\n",
       " 'platform',\n",
       " 'protection',\n",
       " 'ssis integration',\n",
       " 'stringent industry standards',\n",
       " 'take off data',\n",
       " 'tms',\n",
       " 'tool',\n",
       " 'validation',\n",
       " 'work independently',\n",
       " 'web analytics',\n",
       " 'data discovery classification',\n",
       " 'data profiling and prep',\n",
       " 'dba',\n",
       " 'inventory',\n",
       " 'lineage',\n",
       " 'management',\n",
       " 'marketing campaign',\n",
       " 'merging',\n",
       " 'minimal supervision',\n",
       " 'ml-powered',\n",
       " 'ml/ai',\n",
       " 'mobile soc',\n",
       " 'policy',\n",
       " 'real-time database replication',\n",
       " 'roadmap',\n",
       " 'specifications',\n",
       " 'team',\n",
       " 'transactional contexts',\n",
       " 'unsupervised machine learning',\n",
       " 'validate',\n",
       " 'on-premises enterprise data',\n",
       " 'publish',\n",
       " 'real-time',\n",
       " 'unified',\n",
       " 'vli',\n",
       " 'lab',\n",
       " 'migration',\n",
       " 'recruiting',\n",
       " 'mobile',\n",
       " 'normalization',\n",
       " 'ownership',\n",
       " 'presentation',\n",
       " 'prototype',\n",
       " 'recognition',\n",
       " 'refreshes',\n",
       " 'relativity',\n",
       " 'systems engineering',\n",
       " 'tact',\n",
       " 'train',\n",
       " 'value case analysis',\n",
       " 'organization',\n",
       " 'project  management]',\n",
       " 'risk management',\n",
       " 'rtcdp  certification',\n",
       " 'unified stream',\n",
       " 'us military',\n",
       " 'version control',\n",
       " 'design',\n",
       " 'etl',\n",
       " 'judgment',\n",
       " 'medical records',\n",
       " 'metasdata',\n",
       " 'planning',\n",
       " 'prioritize',\n",
       " 'production coding',\n",
       " 'query performance',\n",
       " 'reporting',\n",
       " 'reporting service',\n",
       " 'spearhead',\n",
       " 'thinking',\n",
       " 'us government',\n",
       " 'neural networks',\n",
       " 'oogle sheets-based',\n",
       " 'profiling',\n",
       " 'releases',\n",
       " 'reverse etl',\n",
       " 'source',\n",
       " 'web',\n",
       " 'ssrs',\n",
       " 'it',\n",
       " 'writing',\n",
       " 'reinforcement learning',\n",
       " 'rigorously',\n",
       " 'cleansing',\n",
       " 'cyber',\n",
       " 'processing',\n",
       " 'native programmatic',\n",
       " 'power and performance analysis',\n",
       " 'statistical modeling',\n",
       " 'technical tagging',\n",
       " 'new theories',\n",
       " 'sql editing',\n",
       " 'refine',\n",
       " 'sci eligibility',\n",
       " 'sharable',\n",
       " 'technologies',\n",
       " 'mitigating risk',\n",
       " 'vc',\n",
       " 'task management',\n",
       " 'simultaneously',\n",
       " 'drive outcomes',\n",
       " 'robotics']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skill_word_list = get_snowflake_column(\n",
    "    SF_MATCH_COLUMN_SKILLS,\n",
    "    SF_SKILL_WORD_TABLE,\n",
    "    ACCOUNT,\n",
    "    SF_USER,\n",
    "    PASSWORD,\n",
    "    DATABASE,\n",
    "    WAREHOUSE,\n",
    "    SCHEMA,\n",
    ")\n",
    "\n",
    "skill_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select bbbb from AAAAA\n"
     ]
    }
   ],
   "source": [
    "sf_table_name = \"AAAAA\"\n",
    "sf_column_name = \"bbbb\"\n",
    "\n",
    "print(f\"select {sf_column_name} from {sf_table_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
