{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names\n",
    "#\n",
    "\"\"\"create file and url names\"\"\"\n",
    "\n",
    "\n",
    "def url_next(keyword, location, days, last_id=None, page_count=0):\n",
    "    \"\"\"Create start url for ids if lastId = None.  Otherwise uses id from previous request\"\"\"\n",
    "    url = f\"https://www.indeed.com/jobs?q={keyword}&l={location}&radius=15&sort=date&fromage={days}\"\n",
    "    if page_count == 0:\n",
    "        return url\n",
    "    else:\n",
    "        return f\"{url}&start={page_count * 10}&vjk={last_id}\"\n",
    "\n",
    "\n",
    "def file_name_ids(folder_path, keyword, location):\n",
    "    \"\"\"creates json file name for ids using keyword and location\"\"\"\n",
    "    file_name = f\"{folder_path}/indeedIds_{keyword}_{location}.json\"\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_retrieve\n",
    "\n",
    "\"\"\"Stores and retrieves local files\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def dd(any_list):\n",
    "    \"\"\"Converts list to set then back to list for the purpose of removing duplicates\"\"\"\n",
    "    list_to_set = set(any_list)\n",
    "    return list(list_to_set)\n",
    "\n",
    "\n",
    "def j_load(file_name):\n",
    "    \"\"\"Retreives existing ids or creates new file.  Returns tuple with id list and count.\n",
    "    Dedupes as precaution.\"\"\"\n",
    "    try:\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "            ja = json.load(f)\n",
    "        og_ids = ja[file_name]\n",
    "        og_list_dd = dd(og_ids)\n",
    "        print(f\"Starting with {len(og_list_dd)} existing ids for {file_name}\")\n",
    "        return (len(og_list_dd), og_list_dd)\n",
    "    except (FileNotFoundError, TypeError, KeyError, ValueError):\n",
    "        print(f\"No file found using {file_name} starting fresh.\")\n",
    "        return (0, [])\n",
    "\n",
    "\n",
    "def j_dump(file_name, ids):\n",
    "    \"\"\"saves ids as json\"\"\"\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({file_name: dd(ids)}, f)\n",
    "    print(f\"{len(ids)} ids saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape\n",
    "#\n",
    "# \"\"\"Connection to api and scraping\"\"\"\n",
    "\n",
    "import requests\n",
    "import names\n",
    "\n",
    "\n",
    "# Uses proxies and redirects to automatically scrape id html.  Requires key.\n",
    "def scrape(retries, api_key, url):\n",
    "    \"\"\"request and return html from job listing page with retries\n",
    "    and results printed.  Single thread.  Returned html is used to\n",
    "    make next url.  Returns r\"\"\"\n",
    "    tries = 0\n",
    "    while tries <= retries:\n",
    "        scraper_api = api_key\n",
    "        payload = {\"api_key\": scraper_api, \"url\": url}\n",
    "        print(f\"Calling: {url}\")\n",
    "        r = requests.get(\"http://api.scraperapi.com\", params=payload, timeout=30)\n",
    "        try:\n",
    "            if r.headers[\"sa-statusCode\"] == \"200\":\n",
    "                print(\n",
    "                    f\"{r.headers['Date']}   {r.headers['sa-final-url']} \\\n",
    "                      status: {r.headers['sa-statusCode']}\"\n",
    "                )\n",
    "            return r\n",
    "        except KeyError:\n",
    "            print(f\"{r.headers}\")\n",
    "            tries += 1\n",
    "        print(rf\"Try {tries} of {retries} n\\ {r.headers}\")\n",
    "    return print(f\"{url} has problems\")\n",
    "\n",
    "\n",
    "def calling(keyword, location, days, api_key, last_id, page_count):\n",
    "    \"\"\"Bundles methods and returns id html.\"\"\"\n",
    "    id_url = names.url_next(keyword, location, days, last_id, page_count)\n",
    "    r = scrape(3, api_key, id_url)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids\n",
    "\n",
    "\"\"\"Modules to retrieve and store Indeed Ids\"\"\"\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_ids(r):\n",
    "    \"\"\"convert search page response to list of ids.\"\"\"\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    td_soup = soup.find_all(\"h2\")\n",
    "    jk_list = []\n",
    "    for soup in td_soup:\n",
    "        aa = soup.a\n",
    "        try:\n",
    "            jk = aa.attrs[\"data-jk\"]\n",
    "            jk_list.append(jk)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    return jk_list\n",
    "\n",
    "\n",
    "def get_new_ids(r, file_name, old_ids):\n",
    "    # takes allIds and pageCount from previous loop if available\n",
    "    \"\"\"Returns a uniques set of ids consisting of old ids and this loop.\"\"\"\n",
    "    # loads saved file if first loop\n",
    "\n",
    "    if len(old_ids) == 0:\n",
    "        existing_ids_load = j_load(file_name)\n",
    "        existing_id_count = existing_ids_load[0]\n",
    "        existing_ids = existing_ids_load[1]\n",
    "        existing_id_dd = dd(existing_ids)\n",
    "        print(f\"starting id count is {existing_ids_load[0]}\")\n",
    "    else:\n",
    "        existing_id_dd = dd(old_ids)\n",
    "        existing_id_count = len(existing_id_dd)\n",
    "    try:\n",
    "        loop_id_list = get_ids(r)\n",
    "        all_unique = dd(existing_id_dd + loop_id_list)\n",
    "    except (NameError, TypeError, AttributeError):\n",
    "        all_unique = []\n",
    "    all_unique_count = len(all_unique)\n",
    "    loop_unique_count = all_unique_count - existing_id_count\n",
    "    print(\n",
    "        f\"\\n{dt.now()} page_unique_count: {loop_unique_count} currentCount: {all_unique_count}\"\n",
    "    )\n",
    "    return all_unique\n",
    "\n",
    "\n",
    "def zero_count(loop_count):\n",
    "    \"\"\"Returns false when 0 new ids returned twice in a row.\n",
    "    Used to end scraping\"\"\"\n",
    "    if len(loop_count) <= 2:\n",
    "        return True\n",
    "    if loop_count[-1] + loop_count[-2] > 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def just_ids(keyword=None, location=None, api_key=None, days=None, folder_path=None):\n",
    "    \"\"\"Assembles methods for IDs.  Save and count all.  Keyboard interupt causes end and save\"\"\"\n",
    "    current_id_list = dd([])\n",
    "    loop_count_list = []\n",
    "    page_count = 0\n",
    "    file_name = names.file_name_ids(folder_path, keyword, location)\n",
    "    # breaks loop when 2 consecutive loops return 0 new ids\n",
    "    try:\n",
    "        while zero_count(loop_count_list):\n",
    "            try:\n",
    "                last_id = current_id_list[-1]\n",
    "            except IndexError:\n",
    "                last_id = str()\n",
    "            try:\n",
    "                r = scrape.calling(\n",
    "                    keyword, location, days, api_key, last_id, page_count\n",
    "                )\n",
    "            except urllib3.exceptions.ReadTimeoutError as e:\n",
    "                print(f\"{e}\")\n",
    "                j_dump(file_name, current_id_list)\n",
    "            newest_ids = get_new_ids(r, file_name, current_id_list)\n",
    "            try:\n",
    "                loop_count = len(newest_ids) - len(current_id_list)\n",
    "            except TypeError:\n",
    "                loop_count = len(newest_ids)\n",
    "            loop_count_list.append(loop_count)\n",
    "            page_count += 1\n",
    "            for new in newest_ids:\n",
    "                current_id_list.append(new)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    j_dump(file_name, dd(current_id_list))\n",
    "    print(\n",
    "        f\"Finished!  New id count is {len(dd(current_id_list))} \\\n",
    "        Saved to: {file_name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snot\n",
    "\n",
    "\"\"\"Collection of python snowflake tools\"\"\"\n",
    "\n",
    "import os\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ctx = snowflake.connector.connect(\n",
    "    account=os.getenv(\"ACCOUNT\"),\n",
    "    user=os.getenv(\"SF_USER\"),\n",
    "    password=os.getenv(\"PASSWORD\"),\n",
    "    role=os.getenv(\"ROLE\"),\n",
    "    database=os.getenv(\"DATABASE\"),\n",
    "    warehouse=os.getenv(\"WAREHOUSE\"),\n",
    "    schema=os.getenv(\"SCHEMA\"),\n",
    ")\n",
    "\n",
    "\n",
    "def df_to_table(df=None, table=None) -> dict:\n",
    "    \"\"\"Appends df to snowflake table or creates new table\"\"\"\n",
    "    conn = snowflake.connector.connect(\n",
    "        account=os.getenv(\"ACCOUNT\"),\n",
    "        user=os.getenv(\"SF_USER\"),\n",
    "        password=os.getenv(\"PASSWORD\"),\n",
    "        role=os.getenv(\"ROLE\"),\n",
    "        database=os.getenv(\"DATABASE\"),\n",
    "        warehouse=os.getenv(\"WAREHOUSE\"),\n",
    "        schema=os.getenv(\"SCHEMA\"),\n",
    "    )\n",
    "    str_df = df.astype(str)\n",
    "    success, chunks, rows, snowflake_output = write_pandas(\n",
    "        conn=conn, df=str_df, table_name=table, auto_create_table=True\n",
    "    )\n",
    "    if success is True:\n",
    "        print(f\"Success!  {rows} rows added to {table} in {chunks} chunks\")\n",
    "    else:\n",
    "        print(f\"DID NOT WORK! {table} \\n {snowflake_output}\")\n",
    "\n",
    "\n",
    "def current_ids(table_name=None):\n",
    "    \"\"\"retrieves existing ids to avoid duplication\"\"\"\n",
    "\n",
    "    with snowflake.connector.connect(\n",
    "        account=os.getenv(\"ACCOUNT\"),\n",
    "        user=os.getenv(\"SF_USER\"),\n",
    "        password=os.getenv(\"PASSWORD\"),\n",
    "        role=os.getenv(\"ROLE\"),\n",
    "        database=os.getenv(\"DATABASE\"),\n",
    "        warehouse=os.getenv(\"WAREHOUSE\"),\n",
    "        schema=os.getenv(\"SCHEMA\"),\n",
    "    ) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(f'select \"job_id\" from {table_name}')\n",
    "            id_list = []\n",
    "            for col1 in cur:\n",
    "                idee = col1[0]\n",
    "                id_list.append(idee)\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Remote']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "location = os.getenv(\"LOCATION\").split()\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract\n",
    "\n",
    "\"\"\"Filter and extract job data based on ids\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "scraper_api = os.getenv(\"API_KEY\")\n",
    "\n",
    "\n",
    "def get_job_dict(id_tuple):\n",
    "    \"\"\"Makes api call and converts html response to soup,\n",
    "    extracts specific json, converts to dict return\"\"\"\n",
    "    keyword = id_tuple[0]\n",
    "    location = id_tuple[1]\n",
    "    id_number = id_tuple[2]\n",
    "    url = id_tuple[3]\n",
    "    r = scrape.scrape(3, scraper_api, url).text\n",
    "    try:\n",
    "        # create soup from r\n",
    "        soup = BeautifulSoup(r, \"html.parser\")\n",
    "        # find and extract json from soup\n",
    "        j_soup = soup.find(\"script\", type=\"application/ld+json\").text\n",
    "        # convert json to dict\n",
    "        d_soup = json.loads(j_soup)\n",
    "        # Append data from tuple\n",
    "        d_soup.update(\n",
    "            {\n",
    "                \"jobId\": id_number,\n",
    "                \"jobKeyword\": keyword,\n",
    "                \"jobSearchLocation\": location,\n",
    "            }\n",
    "        )\n",
    "        # print(f'{dt.now()}get_job_dict try')\n",
    "        return d_soup\n",
    "    except AttributeError:\n",
    "        # print(f'{dt.now()}get_job_dict ex')\n",
    "        return {\n",
    "            \"jobId\": id_number,\n",
    "            \"jobKeyword\": keyword,\n",
    "            \"jobSearchLocation\": location,\n",
    "        }\n",
    "\n",
    "\n",
    "# Ids ready\n",
    "def ids_warming(path, table_name):\n",
    "    \"\"\"Extract ids from json files.  Ouput list of tuples with keyword, location, id, URL\"\"\"\n",
    "    id_tuple_list = []\n",
    "    # get ids that have already run\n",
    "    df_ids = current_ids(table_name)\n",
    "    # list of id file names\n",
    "    ids = glob.glob(f\"{path}/indeedIds_*.json\")\n",
    "    # loop through file names extract kw and location\n",
    "    for d in ids:\n",
    "        id_path = d.split(\"/\")[-1]\n",
    "        id_name = id_path.split(\"_\")\n",
    "        kw = id_name[1]\n",
    "        loc = id_name[2].split(\".\")[0]\n",
    "        # open each id file\n",
    "        with open(d, \"r\", encoding=\"utf-8\") as f:\n",
    "            j_ids = json.load(f)[d]\n",
    "            # if id already in df skip\n",
    "            for j in j_ids:\n",
    "                if j in df_ids:\n",
    "                    pass\n",
    "                elif kw in (\"test\", \"write\"):\n",
    "                    pass\n",
    "                # not in df... create tup using (kw,loc,id) and append to list\n",
    "                else:\n",
    "                    url = f\"https://www.indeed.com/m/viewjob?jk={j}\"\n",
    "                    id_tup = (kw, loc, j, url)\n",
    "                    id_tuple_list.append(id_tup)\n",
    "    # return set of all unique unprocessed tuples\n",
    "    tuple_dd_list = dd(id_tuple_list)\n",
    "    print(f\"{len(tuple_dd_list)} unique job ids will be processed\")\n",
    "    return tuple_dd_list\n",
    "\n",
    "\n",
    "# This func does not work as stand alone\n",
    "def li_li_li_list(description):\n",
    "    \"\"\"Takes description dict and returns li values as list\"\"\"\n",
    "    # Resoup dict\n",
    "    d_soup = BeautifulSoup(description, \"html.parser\")\n",
    "    # get list of li tag text\n",
    "    lis = d_soup.find_all(\"li\")\n",
    "    tag_list = []\n",
    "    for l in lis:\n",
    "        # get just li text and put back in list\n",
    "        bullet = l.text\n",
    "        tag_list.append(bullet)\n",
    "    # print(f'{dt.now()}li_li_li_list')\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Design', 'data']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spac\n",
    "\n",
    "\"\"\"Spacy nlp functions\"\"\"\n",
    "\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "stopwords = list(nlp.Defaults.stop_words)\n",
    "\n",
    "\n",
    "def spacy_proper(doc):\n",
    "    \"\"\"Takes string as input and returns a list of Proper Nouns\"\"\"\n",
    "    pn_list = []\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == \"PROPN\":\n",
    "            pn_list.append(tok.text)\n",
    "        else:\n",
    "            pass\n",
    "    return pn_list\n",
    "\n",
    "\n",
    "def sentence_parse_proper(sentences):\n",
    "    \"\"\"Parses list of sentences and returns list of proper nouns\"\"\"\n",
    "    col_lists = []\n",
    "    try:\n",
    "        for sentence in sentences:\n",
    "            ss = sentence.strip()\n",
    "            doc = nlp(ss)\n",
    "            pn = spacy_proper(doc)\n",
    "            col_lists.append(pn)\n",
    "    except ValueError:\n",
    "        col_lists.append([])\n",
    "    return set(chain.from_iterable(col_lists))\n",
    "\n",
    "\n",
    "def pattern_lower(csv_file, column_name):\n",
    "    \"\"\"Formats column from csv file into patterns for matching\"\"\"\n",
    "    pattern_list = []\n",
    "    df = pd.read_csv(csv_file)\n",
    "    word_list = list(df[column_name])\n",
    "    clean_word_list = [str(t).lower().strip() for t in word_list if t is not np.nan]\n",
    "    split_list = [t.split() for t in clean_word_list]\n",
    "    for i in range(len(split_list)):\n",
    "        words = []\n",
    "        sentence = split_list[i]\n",
    "        for w in sentence:\n",
    "            pattern = dict(LOWER=str(w))\n",
    "            words.append(pattern)\n",
    "        pattern_list.append(words)\n",
    "    # print(f'{column_name} now contains {len(pattern_list)} keywords')\n",
    "    return pattern_list\n",
    "\n",
    "\n",
    "def data_word_match(sentence, csv_file, column_name):\n",
    "    \"\"\"Matches phrase patterns\"\"\"\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    word_patterns = pattern_lower(csv_file, column_name)\n",
    "    matcher.add(column_name, word_patterns, greedy=\"FIRST\")\n",
    "    doc = nlp(sentence)\n",
    "    matches = matcher(\n",
    "        doc\n",
    "    )  # [(15794293310264179361, 0, 1), (15794293310264179361, 7, 8)]\n",
    "    words = []\n",
    "    for match_id, start, end in matches:  ##match_id, not used\n",
    "        span = doc[start:end]\n",
    "        words.append(span.text)\n",
    "    return list(words)\n",
    "\n",
    "\n",
    "sentence = \"Design and build new ELT-based data models using SQL and dbt\"\n",
    "csv_file = \"Data/snow_words.csv\"\n",
    "column_name = \"Data_Skills\"\n",
    "data_word_match(sentence, csv_file, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use lemmatization to match base words instead of exact # [fixme]\n",
    "\n",
    "\n",
    "def sentence_parse_data_words(sentences, csv_file, column_name):\n",
    "    \"\"\"Takes string as input and returns a list of Proper Nouns\"\"\"\n",
    "    words_lists = []\n",
    "    try:\n",
    "        for sentence in sentences:\n",
    "            words = data_word_match(sentence, csv_file, column_name)\n",
    "            words_lists.append(words)\n",
    "    except ValueError:\n",
    "        words_lists.append([])\n",
    "    return set(chain.from_iterable(words_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch-dict\n",
    "#\n",
    "# \"\"\"Handles errors and assembles job dict\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def check_and_extract(full_dict):\n",
    "    \"\"\"This function is designed to catch errors in the dictionary.\n",
    "    Function li_li_li_list extracts list of values.  Dict with data and lables returned\n",
    "    \"\"\"\n",
    "    try:\n",
    "        company = full_dict[\"hiringOrganization\"][\"name\"]\n",
    "    except KeyError:\n",
    "        company = \"Unavailable\"\n",
    "    except TypeError:\n",
    "        company = \"Unavailable\"\n",
    "    try:\n",
    "        title = full_dict[\"title\"]\n",
    "    except TypeError:\n",
    "        title = \"Unavailable\"\n",
    "    except KeyError:\n",
    "        title = \"Unavailable\"\n",
    "    try:\n",
    "        base_salary_low = full_dict[\"baseSalary\"][\"value\"][\"minValue\"]\n",
    "    except KeyError:\n",
    "        base_salary_low = \"Unavailable\"\n",
    "    except TypeError:\n",
    "        base_salary_low = \"Unavailable\"\n",
    "    try:\n",
    "        base_salary_high = full_dict[\"baseSalary\"][\"value\"][\"maxValue\"]\n",
    "    except KeyError:\n",
    "        base_salary_high = \"Unavailable\"\n",
    "    except TypeError:\n",
    "        base_salary_high = \"Unavailable\"\n",
    "    try:\n",
    "        salary_period = full_dict[\"baseSalary\"][\"value\"][\"unitText\"]\n",
    "    except KeyError:\n",
    "        salary_period = \"Unavailable\"\n",
    "    except TypeError:\n",
    "        salary_period = \"Unavailable\"\n",
    "    try:\n",
    "        date_posted = full_dict[\"datePosted\"]\n",
    "    except KeyError:\n",
    "        date_posted = \"Unavailable\"\n",
    "    except TypeError:\n",
    "        date_posted = \"Unavailable\"\n",
    "    try:\n",
    "        date_expires = full_dict[\"date_expires\"]\n",
    "    except TypeError:\n",
    "        date_expires = \"Unavailable\"\n",
    "    except KeyError:\n",
    "        date_expires = \"Unavailable\"\n",
    "    try:\n",
    "        employment_type = full_dict[\"employmentType\"]\n",
    "    except KeyError:\n",
    "        employment_type = \"Unavailable\"\n",
    "    except TypeError:\n",
    "        employment_type = \"Unavailable\"\n",
    "    try:\n",
    "        location = full_dict[\"jobLocation\"][\"address\"][\"addressLocality\"]\n",
    "    except KeyError:\n",
    "        location = \"Unavailable\"\n",
    "    except TypeError:\n",
    "        location = \"Unavailable\"\n",
    "    try:\n",
    "        description_raw = full_dict[\"description\"]\n",
    "    except KeyError:\n",
    "        description_raw = \"Unavailable\"\n",
    "    except TypeError:\n",
    "        description_raw = \"Unavailable\"\n",
    "    try:\n",
    "        description_list = li_li_li_list(description_raw)\n",
    "    except KeyError:\n",
    "        description_list = \"Unavailable\"\n",
    "    except TypeError:\n",
    "        description_list = \"Unavailable\"\n",
    "    skills = sentence_parse_data_words(\n",
    "        description_list, \"Data/snow_words.csv\", \"Data_Skills\"\n",
    "    )\n",
    "    data_skills = [x.lower() for x in skills]\n",
    "    technology = sentence_parse_data_words(\n",
    "        description_list, \"Data/snow_words.csv\", \"Data_Technology\"\n",
    "    )\n",
    "    data_technology = [x.lower() for x in technology]\n",
    "    propers = sentence_parse_proper(description_list)\n",
    "    proper_nouns = [\n",
    "        x.lower()\n",
    "        for x in propers\n",
    "        if x.lower() not in data_skills and x.lower() not in data_technology\n",
    "    ]\n",
    "    job_id = full_dict[\"jobId\"]\n",
    "    keyword = full_dict[\"jobKeyword\"]\n",
    "    search_location = full_dict[\"jobSearchLocation\"]\n",
    "    print(\n",
    "        f\"{job_id}  {keyword}   {search_location}  {company}  {title}  {date_posted}\\\n",
    "          {employment_type}  {location}  \\n  {proper_nouns} \\n {data_skills} \\n {data_technology}\"\n",
    "    )\n",
    "    job_dict = {\n",
    "        \"job_id\": job_id,\n",
    "        \"entered\": time.strftime(\"%D %T\"),\n",
    "        \"search_keyword\": keyword,\n",
    "        \"search_location\": search_location,\n",
    "        \"job_company\": company,\n",
    "        \"job_title\": title,\n",
    "        \"job_date_posted\": date_posted,\n",
    "        \"job_date_expires\": date_expires,\n",
    "        \"pay_low\": base_salary_low,\n",
    "        \"pay_high\": base_salary_high,\n",
    "        \"pay_period\": salary_period,\n",
    "        \"job_type\": employment_type,\n",
    "        \"job_location\": location,\n",
    "        \"description_sentences\": description_list,\n",
    "        \"proper_nouns\": proper_nouns,\n",
    "        \"data_skills\": data_skills,\n",
    "        \"data_technology\": data_technology,\n",
    "    }\n",
    "    return job_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words\n",
    "\n",
    "\"\"\"Combines and executes word functions to create final output\"\"\"\n",
    "\n",
    "import traceback\n",
    "import time\n",
    "import pandas as pd\n",
    "import extract\n",
    "import catch_dict\n",
    "import snot\n",
    "\n",
    "now = int(time.time() * 1000000)\n",
    "\n",
    "\n",
    "def come_together(path, table_name):\n",
    "    \"\"\"runs scraped ids and returns list df\"\"\"\n",
    "    dict_list = []\n",
    "    tup_list = extract.ids_warming(\n",
    "        path, table_name\n",
    "    )  # returns list of id tuples  ->list\n",
    "    er_count = 0\n",
    "    try:\n",
    "        for tup in tup_list:\n",
    "            try:\n",
    "                job_dict = extract.get_job_dict(\n",
    "                    tup\n",
    "                )  # takes one id tuple and outputs dict with data -> dict\n",
    "                new_job_dict = catch_dict.check_and_extract(\n",
    "                    job_dict\n",
    "                )  #  (description_list:[]) -> dict\n",
    "                # should only run ids not already in table\n",
    "                dict_list.append(new_job_dict)\n",
    "                print(f\"{len(dict_list)} jobs out of {len(tup_list)} processed\")\n",
    "            except TimeoutError:\n",
    "                traceback.print_exc()\n",
    "                er_count += 1\n",
    "                er_retry = er_count * 60\n",
    "                print(f\"Error has occured.  Will retry in {er_retry}\")\n",
    "        df_new = pd.DataFrame(dict_list)\n",
    "        snot.df_to_table(df_new, table_name)\n",
    "        return df_new\n",
    "    except KeyboardInterrupt:\n",
    "        df_new = pd.DataFrame(dict_list)\n",
    "        snot.df_to_table(df_new, table_name)\n",
    "        return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxxxxx'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def x_obscure(string):\n",
    "    letters = []\n",
    "    for s in string:\n",
    "        letters.append(\"x\")\n",
    "    xx = \"\".join(letters)\n",
    "    return xx\n",
    "\n",
    "\n",
    "x_obscure(\"string\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
