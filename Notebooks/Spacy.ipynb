{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# import spacy_annotator as spa\n",
    "from spacy.kb import InMemoryLookupKB\n",
    "import spacy\n",
    "from string import punctuation\n",
    "\n",
    "cwd = Path.cwd()\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 0 0 h i r e s',\n",
       " '1 0 x w i n n e r s',\n",
       " '1 2 d',\n",
       " '1 f l o w',\n",
       " '1 f o r t',\n",
       " '1 w o r l d s y n c',\n",
       " '2 4 o r m',\n",
       " '3 3 a c r o s s',\n",
       " '3 6 0',\n",
       " '3 6 5',\n",
       " '3 6 5 t a l e n t s',\n",
       " '3 d m o d e l s p a c e',\n",
       " '3 d a t a',\n",
       " '3 m',\n",
       " '3 r a d i c a l',\n",
       " '4 0 9 a',\n",
       " '4 2',\n",
       " '4 2 m a t t e r s',\n",
       " '4 a l l p o r t a l',\n",
       " '4 e a s y s o f t',\n",
       " '4 m e d i c a',\n",
       " '5 0 0 m a i l',\n",
       " '6 s e n s e',\n",
       " '7',\n",
       " '8 0 l e g s',\n",
       " '9',\n",
       " '@ e a s e',\n",
       " 'a - i n s i g h t s',\n",
       " 'a - l i g n',\n",
       " 'a - q u a',\n",
       " 'a - s c e n d',\n",
       " 'a 2 x',\n",
       " 'a b b',\n",
       " 'a b e l d e n t',\n",
       " 'a c t i v e w o r k s',\n",
       " 'a c t s',\n",
       " 'a d',\n",
       " 'a d c +',\n",
       " 'a d p',\n",
       " 'a e s',\n",
       " 'a f s',\n",
       " 'a i f m d',\n",
       " 'a i g e n e s i s',\n",
       " 'a i m l e a p',\n",
       " 'a i n a',\n",
       " 'a m l',\n",
       " 'a m z',\n",
       " 'a n t i - m o n e y',\n",
       " 'a o m e i',\n",
       " 'a p a r a v i ,',\n",
       " 'a p b a c k u p',\n",
       " 'a p i a n t',\n",
       " 'a p i s c r a p y',\n",
       " 'a p o s',\n",
       " 'a p p s e c o n n e c t',\n",
       " 'a p t a n i a',\n",
       " 'a r c o n',\n",
       " 'a r c o s',\n",
       " 'a r i e s',\n",
       " 'a r m s',\n",
       " 'a r t i k',\n",
       " 'a s c',\n",
       " 'a s i n a s i n',\n",
       " 'a s n a',\n",
       " 'a s o',\n",
       " 'a s p e n - r m s',\n",
       " 'a t l a s . t i',\n",
       " 'a t l o s',\n",
       " 'a t \\\\ u 0 0 2 6 t',\n",
       " 'a v e v a',\n",
       " 'a v g',\n",
       " 'a v l',\n",
       " 'a w s',\n",
       " 'a y l i e n',\n",
       " 'a a a m e e t . i n',\n",
       " 'a b',\n",
       " 'a b a c u m',\n",
       " 'a b l a',\n",
       " 'a b s o l u t e',\n",
       " 'a b y s s a l e',\n",
       " 'a c a s t a t',\n",
       " 'a c a d l e',\n",
       " 'a c c e l d a t a',\n",
       " 'a c c e n t u r e',\n",
       " 'a c c e s s',\n",
       " 'a c c i o',\n",
       " 'a c c l a i m',\n",
       " 'a c c o m p l i s h',\n",
       " 'a c c o p s',\n",
       " 'a c c o r d',\n",
       " 'a c c u m a i l',\n",
       " 'a c c u s t o r e',\n",
       " 'a c c u m u l o',\n",
       " 'a c e',\n",
       " 'a c h i e v e',\n",
       " 'a c h o',\n",
       " 'a c m e',\n",
       " 'a c o d i s',\n",
       " 'a c q u i a',\n",
       " 'a c r o n i s',\n",
       " 'a c r y l',\n",
       " 'a c t e r y s',\n",
       " 'a c t i a n',\n",
       " 'a c t i f i o',\n",
       " 'a c t i o n i q',\n",
       " 'a c t i t o',\n",
       " 'a c t i v',\n",
       " 'a c t i v a t i o n',\n",
       " 'a c t i v e',\n",
       " 'a c t i v e @ d i s k',\n",
       " 'a c t i v e b a t c h',\n",
       " 'a c t i v e p r i m e',\n",
       " 'a c u a t i v e',\n",
       " 'a d',\n",
       " 'a d l i b',\n",
       " 'a d s t a g e',\n",
       " 'a d s y s t e m',\n",
       " 'a d v a l u e',\n",
       " 'a d a l y z',\n",
       " 'a d a p t i v e',\n",
       " 'a d a s t r a',\n",
       " 'a d d e n d',\n",
       " 'a d d r e s s',\n",
       " 'a d d r e s s z e n',\n",
       " 'a d e l e',\n",
       " 'a d e o n a',\n",
       " 'a d e p t',\n",
       " 'a d f o r m',\n",
       " 'a d m e r s',\n",
       " 'a d m e t r i c k s',\n",
       " 'a d m i n',\n",
       " 'a d m i x e r . d s p',\n",
       " 'a d o b e',\n",
       " 'a d s q u a r e',\n",
       " 'a d v a n c e',\n",
       " 'a d v a n c e f l o w',\n",
       " 'a d v a n c e d',\n",
       " 'a d v a n c e d m i n e r',\n",
       " 'a d v a n t a g e',\n",
       " 'a d v e r i t y',\n",
       " 'a d v i z o r p r o',\n",
       " 'a e q i u m',\n",
       " 'a e r i e s',\n",
       " 'a e r o s p i k e',\n",
       " 'a f f i n d a',\n",
       " 'a f f i n i t e x t',\n",
       " 'a f f i n o',\n",
       " 'a f f i s e',\n",
       " 'a f f o g a t a',\n",
       " 'a f f o r a i',\n",
       " 'a g',\n",
       " 'a g a i n',\n",
       " 'a g e n c y',\n",
       " 'a g e n c y a n a l y t i c s',\n",
       " 'a g e n c y o r g a n i z e r',\n",
       " 'a g e n d a',\n",
       " 'a g e n t n o o n',\n",
       " 'a g g r e g a t e',\n",
       " 'a g g u a',\n",
       " 'a g i c a p',\n",
       " 'a g i l e r m e',\n",
       " 'a g i l i o',\n",
       " 'a g i l i t y',\n",
       " 'a g i l y s y s',\n",
       " 'a g i s o f t',\n",
       " 'a g n i',\n",
       " 'a h s h a y',\n",
       " 'a i p o r t a l',\n",
       " 'a i b i d i a',\n",
       " 'a i d a f o r m',\n",
       " 'a i r c h e c k',\n",
       " 'a i r d a t a',\n",
       " 'a i r v a n t a g e',\n",
       " 'a i r a p i',\n",
       " 'a i r b y t e',\n",
       " 'a i r c l o a k',\n",
       " 'a i r c r a f t',\n",
       " 'a i r m e e t',\n",
       " 'a i r p o r t t r a n s f e r . c o m',\n",
       " 'a i r s l i p',\n",
       " 'a i r s t o r y',\n",
       " 'a i s e e s o f t',\n",
       " 'a i s e r a',\n",
       " 'a i v e n',\n",
       " 'a k a m a i',\n",
       " 'a k e n e o',\n",
       " 'a k k i o',\n",
       " 'a k k u',\n",
       " 'a k t o',\n",
       " 'a l a t i o n',\n",
       " 'a l a v i . a i',\n",
       " 'a l b a c r o s s',\n",
       " 'a l b a t r o s s',\n",
       " 'a l c h e m e r',\n",
       " 'a l c h e m y',\n",
       " 'a l c i o n',\n",
       " 'a l g o d o c s',\n",
       " 'a l g o s o n e',\n",
       " 'a l i b a b a',\n",
       " 'a l i g n . l y',\n",
       " 'a l l',\n",
       " 'a l l v o i c e s',\n",
       " 'a l l e g r o',\n",
       " 'a l l i a n c e',\n",
       " 'a l l o',\n",
       " 'a l l o y',\n",
       " 'a l l o y . a i',\n",
       " 'a l l s t a c k s',\n",
       " 'a l m a',\n",
       " 'a l o o b a',\n",
       " 'a l p h a',\n",
       " 'a l p h a s e n s e',\n",
       " 'a l p i n e',\n",
       " 'a l t s o u r c e',\n",
       " 'a l t t e x t . a i',\n",
       " 'a l t a',\n",
       " 'a l t a p o i n t',\n",
       " 'a l t a i r',\n",
       " 'a l t e r a',\n",
       " 'a l t e r i a',\n",
       " 'a l t e r y x',\n",
       " 'a l t i b a s e',\n",
       " 'a l t i u m',\n",
       " 'a l t o v a',\n",
       " 'a l t v i a',\n",
       " 'a l u m i o',\n",
       " 'a m a k a',\n",
       " 'a m a l i a',\n",
       " 'a m a z i n g h i r i n g',\n",
       " 'a m a z o n',\n",
       " 'a m b e r',\n",
       " 'a m b e r l e a f',\n",
       " 'a m b i r e',\n",
       " 'a m b r a',\n",
       " 'a m i g o c l o u d',\n",
       " 'a m o b e e',\n",
       " 'a m p e r i t y',\n",
       " 'a m p l i f a i',\n",
       " 'a m p l i f y',\n",
       " 'a m p l i z',\n",
       " 'a n a c o n d a',\n",
       " 'a n a l a n c e ™',\n",
       " 'a n a l y t i c s',\n",
       " 'a n a l y t i c s 4 n o w',\n",
       " 'a n a l y t i c s b o x',\n",
       " 'a n a l y t i c s c r e a t o r',\n",
       " 'a n a l y t i c s v e r s e',\n",
       " 'a n a l y t i x l a b s',\n",
       " 'a n c h a n t o',\n",
       " 'a n c h o r',\n",
       " 'a n g l e s',\n",
       " 'a n g o',\n",
       " 'a n g u l a r',\n",
       " 'a n n o y',\n",
       " 'a n o m a l o',\n",
       " 'a n o n o s',\n",
       " 'a n s a r a d a',\n",
       " 'a n s w e r d o c k',\n",
       " 'a n s w e r m i n e r',\n",
       " 'a n s y s',\n",
       " 'a n t e r i a d',\n",
       " 'a n t h o l o g y',\n",
       " 'a n u r a . i o',\n",
       " 'a n v e o',\n",
       " 'a n v i l',\n",
       " 'a n v i z e n t',\n",
       " 'a n v y l',\n",
       " 'a n y b i z . i o',\n",
       " 'a n y c h a r t',\n",
       " 'a n y g a n t t',\n",
       " 'a n y m p 4',\n",
       " 'a n y m a p',\n",
       " 'a n y r o a d',\n",
       " 'a n y s t o c k',\n",
       " 'a n z o',\n",
       " 'a p a c h e',\n",
       " 'a p e a k s o f t',\n",
       " 'a p e r t u r e',\n",
       " 'a p e x s q l',\n",
       " 'a p i d e c k',\n",
       " 'a p i f y',\n",
       " 'a p i g e e',\n",
       " 'a p l y n k',\n",
       " 'a p o l l o',\n",
       " 'a p o n o',\n",
       " 'a p o r i a',\n",
       " 'a p p g i n i',\n",
       " 'a p p m a s t e r . i o',\n",
       " 'a p p n e x u s',\n",
       " 'a p p o m n i',\n",
       " 'a p p s e a l i n g',\n",
       " 'a p p a r i t y',\n",
       " 'a p p d o m e',\n",
       " 'a p p e n',\n",
       " 'a p p f l o w . a i',\n",
       " 'a p p g a t e',\n",
       " 'a p p i f y',\n",
       " 'a p p l i e d',\n",
       " 'a p p o r i o',\n",
       " 'a p p r a n i x',\n",
       " 'a p p r i s s',\n",
       " 'a p p s 4 r e n t',\n",
       " 'a p p s f l y e r',\n",
       " 'a p p s e m b l e r',\n",
       " 'a p p t i o',\n",
       " 'a p p t o p i a',\n",
       " 'a p r i v a',\n",
       " 'a p t e a n',\n",
       " 'a p t e c o',\n",
       " 'a p t i t u d e',\n",
       " 'a p t o',\n",
       " 'a p t u m',\n",
       " 'a p t y',\n",
       " 'a q u a',\n",
       " 'a q u a n t',\n",
       " 'a r a n g o d b',\n",
       " 'a r a q i c h',\n",
       " 'a r a s',\n",
       " 'a r c g i s',\n",
       " 'a r c s u r v e y',\n",
       " 'a r c a d e',\n",
       " 'a r c a d i a',\n",
       " 'a r c h i g r a p h',\n",
       " 'a r c h i e . a i',\n",
       " 'a r c h i w a r e',\n",
       " 'a r c h l e t',\n",
       " 'a r c s e r v e',\n",
       " 'a r d o q',\n",
       " 'a r e n a',\n",
       " 'a r e n a . i m',\n",
       " 'a r g o s ™',\n",
       " 'a r i c e n t',\n",
       " 'a r i m o',\n",
       " 'a r i s t a',\n",
       " 'a r i s t o t l e i n s i g h t',\n",
       " 'a r i t i c',\n",
       " 'a r o u n d d e a l',\n",
       " 'a r r c u s',\n",
       " 'a r r i a',\n",
       " 'a r r o w',\n",
       " 'a r t e m i s',\n",
       " 'a r t i c l e',\n",
       " 'a r t l o g i c',\n",
       " 'a r t w o r k f l o w',\n",
       " 'a r u b a',\n",
       " 'a r v i a',\n",
       " 'a r x s p a n',\n",
       " 'a r y s o n',\n",
       " 'a s c e n d',\n",
       " 'a s c e n d a n c e',\n",
       " 'a s h a m p o o',\n",
       " 'a s i g r a',\n",
       " 'a s k d a t a',\n",
       " 'a s k u i t y',\n",
       " 'a s p e c t',\n",
       " 'a s p e c t u m',\n",
       " 'a s p e n',\n",
       " 'a s s e m b l e',\n",
       " 'a s s e m b l y',\n",
       " 'a s s e t',\n",
       " 'a s s e t f u t u r e',\n",
       " 'a s s u r e',\n",
       " 'a s t e r a',\n",
       " 'a s t r a',\n",
       " 'a s t r a c h a t',\n",
       " 'a s t r e a',\n",
       " 'a s t r e l l a',\n",
       " 'a s t r o',\n",
       " 'a s u r i n t',\n",
       " 'a t d a t a',\n",
       " 'a t a c c a m a',\n",
       " 'a t h e n i a n',\n",
       " 'a t h e n i c',\n",
       " 'a t h e n n i a n',\n",
       " 'a t h e n t a',\n",
       " 'a t l a n',\n",
       " 'a t l a s',\n",
       " 'a t l a s o p e n',\n",
       " 'a t l a s s i a n',\n",
       " 'a t o m l i',\n",
       " 'a t t e n d h r m',\n",
       " 'a t t e n d a y s',\n",
       " 'a t t e n t i o n',\n",
       " 'a u d i e n c e p l u s',\n",
       " 'a u d i e n c e s',\n",
       " 'a u d i t b o a r d',\n",
       " 'a u d i t r u n n e r',\n",
       " 'a u d i t s',\n",
       " 'a u g u s t a',\n",
       " 'a u m n i',\n",
       " 'a u r i g o',\n",
       " 'a u r o c r m',\n",
       " 'a u t h 0',\n",
       " 'a u t o',\n",
       " 'a u t o - u p d a t e',\n",
       " 'a u t o / m a t e',\n",
       " 'a u t o e n t r y',\n",
       " 'a u t o m a t c h',\n",
       " 'a u t o r a b i t',\n",
       " 'a u t o r e k',\n",
       " 'a u t o v u e',\n",
       " 'a u t o d e s k',\n",
       " 'a u t o m a t i o n',\n",
       " 'a v a n a n',\n",
       " 'a v a s t',\n",
       " 'a v a y a',\n",
       " 'a v e p d f',\n",
       " 'a v e p o i n t',\n",
       " 'a v e n u e',\n",
       " 'a v e r i c k m e d i a',\n",
       " 'a v e t t a',\n",
       " 'a v i',\n",
       " 'a v i s e',\n",
       " 'a v o',\n",
       " 'a v o l u t i o n',\n",
       " 'a v o n d a t a',\n",
       " 'a v o r a',\n",
       " 'a v r i o',\n",
       " 'a v v o k a',\n",
       " 'a w a r e',\n",
       " 'a x c i e n t',\n",
       " 'a x i a d',\n",
       " 'a x i a l 3 d',\n",
       " 'a x i b a s e',\n",
       " 'a x i s b a s e',\n",
       " 'a x t r i a',\n",
       " 'a x w a y',\n",
       " 'a z o r',\n",
       " 'a z u r e',\n",
       " 'b - m e t r i c s',\n",
       " 'b - s c a d a',\n",
       " 'b 2 b s i g n a l s',\n",
       " 'b b v a',\n",
       " 'b c c',\n",
       " 'b d r s u i t e',\n",
       " 'b e t p l u s',\n",
       " 'b e x e l',\n",
       " 'b f r a m e',\n",
       " 'b i',\n",
       " 'b i a s',\n",
       " 'b i g - i p',\n",
       " 'b i m a c h i n e',\n",
       " 'b i o v i a',\n",
       " 'b i p p',\n",
       " 'b k f',\n",
       " 'b l a z e',\n",
       " 'b m c',\n",
       " 'b o m',\n",
       " 'b o t w i s e',\n",
       " 'b w t',\n",
       " 'b a b e l w a y',\n",
       " 'b a c k b o n e',\n",
       " 'b a c k g r o u n d',\n",
       " 'b a c k r i g h t u p',\n",
       " 'b a c k t r a c e | s a u c e',\n",
       " 'b a c k u p',\n",
       " 'b a c k u p 4 a l l',\n",
       " 'b a c k u p o u t l o o k',\n",
       " 'b a c k u p v a u l t',\n",
       " 'b a c k u p i f y',\n",
       " 'b a c u l a',\n",
       " 'b a f f l e',\n",
       " 'b a k e r f i e l d',\n",
       " 'b a m b o o b o x',\n",
       " 'b a n j o',\n",
       " 'b a n k',\n",
       " 'b a n k s e n s e',\n",
       " 'b a n k r u p t c y w a t c h',\n",
       " 'b a n n e r n o w',\n",
       " 'b a r r a i s e r',\n",
       " 'b a r r a c u d a',\n",
       " 'b a r r e d',\n",
       " 'b a s e',\n",
       " 'b a s e 6 4 . a i :',\n",
       " 'b a s i c a i',\n",
       " 'b a s i s',\n",
       " 'b a t c h d a t a',\n",
       " 'b a t c h l e a d s',\n",
       " 'b a t c h s k i p t r a c i n g',\n",
       " 'b a t t e r y t e c h',\n",
       " 'b e m y e y e',\n",
       " 'b e s m a r t e e',\n",
       " 'b e a n s',\n",
       " 'b e a u h u r s t',\n",
       " 'b e e k e t i n g',\n",
       " 'b e h f a l a b',\n",
       " 'b e l l a d a t i',\n",
       " 'b e n c h s c i',\n",
       " 'b e n c h l i n g',\n",
       " 'b e n e l i n x',\n",
       " 'b e r k e l e y',\n",
       " 'b e r r y',\n",
       " 'b e s t',\n",
       " 'b e t t e r c l o u d',\n",
       " 'b e t t e r c o m p',\n",
       " 'b e t t e r c o n t a c t',\n",
       " 'b e y o n d',\n",
       " 'b e y o n d e r p',\n",
       " 'b e z l i o',\n",
       " 'b i c x o',\n",
       " 'b i g',\n",
       " 'b i d n e t',\n",
       " 'b i d p r i m e',\n",
       " 'b i d v e r t i s e r',\n",
       " 'b i d t h e a t r e',\n",
       " 'b i d t r a c e r',\n",
       " 'b i g',\n",
       " 'b i g d a t a c l o u d',\n",
       " 'b i g i d',\n",
       " 'b i g m l',\n",
       " 'b i g p a n d a',\n",
       " 'b i g e y e',\n",
       " 'b i l d',\n",
       " 'b i l f l o',\n",
       " 'b i n a r b a s e',\n",
       " 'b i n f e r',\n",
       " 'b i n g',\n",
       " 'b i o k y',\n",
       " 'b i o m e t r i c',\n",
       " 'b i o s i t e',\n",
       " 'b i r d',\n",
       " 'b i r d s e y e p m',\n",
       " 'b i t m a r k',\n",
       " 'b i t a m',\n",
       " 'b i t d e f e n d e r',\n",
       " 'b i t g e t',\n",
       " 'b i t g l a s s',\n",
       " 'b i t q u e r y',\n",
       " 'b i t s i g h t',\n",
       " 'b i t s y',\n",
       " 'b i z d a t a x',\n",
       " 'b i z d i r e c t',\n",
       " 'b i z p r o s p e x',\n",
       " 'b i z s c h e d u l e r',\n",
       " 'b i z t a l k 3 6 0',\n",
       " 'b i z b i r d',\n",
       " 'b i z b i z e',\n",
       " 'b i z z a b o',\n",
       " 'b i z z y',\n",
       " 'b l a c k b e r r y',\n",
       " 'b l a c k f o g',\n",
       " 'b l a c k b a u d',\n",
       " 'b l a d e l o g i c',\n",
       " 'b l a n c c o',\n",
       " 'b l a s t p o i n t',\n",
       " 'b l a z e g r a p h',\n",
       " 'b l e n d o o r',\n",
       " 'b l e s t a',\n",
       " 'b l i n k',\n",
       " 'b l i t z e n',\n",
       " 'b l o c k',\n",
       " 'b l o c k s t a c k',\n",
       " 'b l o g d a s h',\n",
       " 'b l o o k e t',\n",
       " 'b l o o m',\n",
       " 'b l o o m b e r g',\n",
       " 'b l o o m r e a c h',\n",
       " 'b l u v a u l t',\n",
       " 'b l u e',\n",
       " 'b l u e c o n i c',\n",
       " 'b l u e c o n n e c t',\n",
       " 'b l u e f l e t c h',\n",
       " 'b l u e o c e a n',\n",
       " 'b l u e s k y',\n",
       " 'b l u e v u',\n",
       " 'b l u e c r e w',\n",
       " 'b l u e m e t e o r',\n",
       " 'b l u e n o d',\n",
       " 'b l u e s h i f t',\n",
       " 'b l u e t i c k',\n",
       " 'b l u r r i f y',\n",
       " 'b o b',\n",
       " 'b o a r d e x',\n",
       " 'b o b s l e d',\n",
       " 'b o i l i n g d a t a',\n",
       " 'b o l t d b',\n",
       " 'b o m b o r a',\n",
       " 'b o n r e p u b l i c',\n",
       " 'b o n z a i',\n",
       " 'b o o k',\n",
       " 'b o o k y o u r d a t a',\n",
       " 'b o o k m a r k',\n",
       " 'b o o k s',\n",
       " 'b o o m i',\n",
       " 'b o o s t e r b e r g',\n",
       " 'b o o t s t r a p',\n",
       " 'b o s s',\n",
       " 'b o t s c r a p e r',\n",
       " 'b o t c o . a i',\n",
       " 'b o t f u e l',\n",
       " 'b o t i f y',\n",
       " 'b o w t i e',\n",
       " 'b o x',\n",
       " 'b r a n d',\n",
       " 'b r a n d i d e a',\n",
       " 'b r a n d n a v',\n",
       " 'b r a n d q u a d',\n",
       " 'b r a n d t t i t u d e',\n",
       " 'b r a n d w i s e',\n",
       " 'b r a v e l y',\n",
       " 'b r e e z e',\n",
       " 'b r e v i t a z',\n",
       " 'b r i c s y s',\n",
       " 'b r i d g e 2 4',\n",
       " 'b r i d g e l o g i c',\n",
       " 'b r i g h t',\n",
       " 'b r i g h t a n a l y t i c s',\n",
       " 'b r i g h t g a u g e',\n",
       " 'b r i g h t g u a g e',\n",
       " 'b r i g h t e r',\n",
       " 'b r i g h t m e t r i c s',\n",
       " 'b r i g h t s t a r d b',\n",
       " 'b r i k l',\n",
       " 'b r i l l i a n t',\n",
       " 'b r i n q a',\n",
       " 'b r o a d b e a n',\n",
       " 'b r o k e r a g e b u i l d e r',\n",
       " 'b r o w s e',\n",
       " 'b r u i n',\n",
       " 'b r y q',\n",
       " 'b u k u',\n",
       " 'b u c k s e n s e',\n",
       " 'b u d',\n",
       " 'b u d d h a',\n",
       " 'b u g h e r d',\n",
       " 'b u g r e p l a y',\n",
       " 'b u i l d a i',\n",
       " 'b u i l d o t s',\n",
       " 'b u i l d r',\n",
       " 'b u i l t w i t h',\n",
       " 'b u m b l e b e e',\n",
       " 'b u s h e l',\n",
       " 'b u s i n e s s',\n",
       " 'b u s i n e s s r a d a r',\n",
       " 'b u x t o n',\n",
       " 'b u z z',\n",
       " 'b u z z c a s t',\n",
       " 'b y a l l a c c o u n t s',\n",
       " 'b y p a t h',\n",
       " 'b y n d e r',\n",
       " 'b y t e b r i d g e',\n",
       " 'b y t e s v i e w',\n",
       " 'c + + b u i l d e r',\n",
       " 'c - p h r a s e',\n",
       " 'c - t r a c k',\n",
       " 'c 2',\n",
       " 'c 2 m',\n",
       " 'c a',\n",
       " 'c a c h a t t o',\n",
       " 'c a d',\n",
       " 'c a d l i n k',\n",
       " 'c a d m a t i c',\n",
       " 'c a d w o r x',\n",
       " 'c a d b r o',\n",
       " 'c a l l r e v u',\n",
       " 'c a m - t o o l',\n",
       " 'c a m 3 5 0',\n",
       " 'c a m s',\n",
       " 'c a n d d i',\n",
       " 'c a r e t',\n",
       " 'c a r t o',\n",
       " 'c a t t',\n",
       " 'c a t p o o l - p c m',\n",
       " 'c b',\n",
       " 'c b 4',\n",
       " 'c d o c',\n",
       " 'c d w',\n",
       " 'c d a t a',\n",
       " 'c i',\n",
       " 'c i e r t o',\n",
       " 'c i m c o n',\n",
       " 'c l o u d b a s i c',\n",
       " 'c m s 2 c m s',\n",
       " 'c m i c',\n",
       " 'c n e x',\n",
       " 'c o d e s y s',\n",
       " 'c o i n s',\n",
       " 'c o m c a s h',\n",
       " 'c o p l i n k',\n",
       " 'c o z y r o c',\n",
       " 'c r d',\n",
       " 'c r e s t',\n",
       " 'c r m',\n",
       " 'c r m i t',\n",
       " 'c s i r o a d',\n",
       " 'c s t o r e o f f i c e',\n",
       " 'c t',\n",
       " 'c u b o',\n",
       " 'c u f i n d e r',\n",
       " 'c x l',\n",
       " 'c y r i s m a',\n",
       " 'c a d a c t i v e',\n",
       " 'c a d c a m',\n",
       " 'c a d d y',\n",
       " 'c a l i b e r m i n d',\n",
       " 'c a l i s o',\n",
       " 'c a l i x a',\n",
       " 'c a l l',\n",
       " 'c a l l f i n d e r',\n",
       " 'c a l l j o u r n e y',\n",
       " 'c a l l t r a x',\n",
       " 'c a l l v i e w',\n",
       " 'c a l l z e n . a i',\n",
       " 'c a l l b o x',\n",
       " 'c a l l i s t o',\n",
       " 'c a l l y z e r',\n",
       " 'c a l y x',\n",
       " 'c a m p a i g n',\n",
       " 'c a m p a i g n a l y z e r',\n",
       " 'c a m p a i g n t r a c k l y',\n",
       " 'c a n i r a n k',\n",
       " 'c a n a d i a n',\n",
       " 'c a n d i d d a t a',\n",
       " 'c a n d i d a t e z i p',\n",
       " 'c a n o p y',\n",
       " 'c a p b',\n",
       " 'c a p l i n k e d',\n",
       " 'c a p e n c y',\n",
       " 'c a p g e m i n i',\n",
       " 'c a p l e n a',\n",
       " 'c a p t a i n',\n",
       " 'c a p t i v 8',\n",
       " 'c a p t u r e 9 1 1',\n",
       " 'c a p t u r i',\n",
       " 'c a r b o n',\n",
       " 'c a r b o n i t e',\n",
       " 'c a r d c o n n e c t',\n",
       " 'c a r d a t a',\n",
       " 'c a r d b o a r d',\n",
       " 'c a r d l y t i c s',\n",
       " 'c a r g o e s',\n",
       " 'c a r h i r e',\n",
       " 'c a r i n g o',\n",
       " 'c a r t i n s i g h t',\n",
       " 'c a r t a',\n",
       " 'c a s e d i r e c t o r p r o',\n",
       " 'c a s e f l e e t',\n",
       " 'c a s e w a r e',\n",
       " 'c a s h p o i n t',\n",
       " 'c a s h i e r l i v e',\n",
       " 'c a s p i o',\n",
       " 'c a s s a n d r a',\n",
       " 'c a s t o r',\n",
       " 'c a s t o r d o c',\n",
       " 'c a t a l o g',\n",
       " 'c a t a l o g f o r c e',\n",
       " 'c a t a l o g u e',\n",
       " 'c a t a l y s t',\n",
       " 'c a t a p u l t',\n",
       " 'c a t c h p o i n t',\n",
       " 'c a u s a l',\n",
       " 'c a v a l l o',\n",
       " 'c a y e n',\n",
       " 'c a y l e y',\n",
       " 'c e c u r i n g',\n",
       " 'c e l i g o',\n",
       " 'c e l l s e l l',\n",
       " 'c e l l e b r i t e',\n",
       " 'c e l o',\n",
       " 'c e n d y n',\n",
       " 'c e n s u s',\n",
       " 'c e n t e r',\n",
       " 'c e n t i m e',\n",
       " 'c e n t r a l',\n",
       " 'c e n t r a l b o s',\n",
       " 'c e n t r a l s q u a r e',\n",
       " 'c e n t r e s t a c k',\n",
       " 'c e n t r e o n',\n",
       " 'c e n t r i c',\n",
       " 'c e n t r i c i t y',\n",
       " 'c e n t r i f u g e',\n",
       " 'c e n t r i p e t a l',\n",
       " 'c e n t r o',\n",
       " 'c e n t r o i d',\n",
       " 'c e r t a',\n",
       " 'c e r t a i n',\n",
       " 'c e r t d o x',\n",
       " 'c e r t o p u s',\n",
       " 'c e r v i n o d a t a',\n",
       " 'c h a n g e',\n",
       " 'c h a n g e n e r d',\n",
       " 'c h a n n a b l e',\n",
       " 'c h a n n e l',\n",
       " 'c h a n n e l a p e',\n",
       " 'c h a n n e l e n g i n e',\n",
       " 'c h a n n e l m i x',\n",
       " 'c h a n n e l r e p l y',\n",
       " 'c h a o s s e a r c h',\n",
       " 'c h a r t . j s',\n",
       " 'c h a r t b l o c k s',\n",
       " 'c h a r t h o p',\n",
       " 'c h a r t m o g u l',\n",
       " 'c h a r t b o o s t',\n",
       " 'c h a r t e r l o g',\n",
       " 'c h a r t i o',\n",
       " 'c h a r t i s t',\n",
       " 'c h a r t m e t r i c',\n",
       " 'c h a r t s',\n",
       " 'c h a t d o x',\n",
       " 'c h a t n b x',\n",
       " 'c h a t b o a t',\n",
       " 'c h a t r h u b',\n",
       " 'c h a t t e r',\n",
       " 'c h a t t e r m i l l',\n",
       " 'c h e c k',\n",
       " 'c h e c k m a r k ™',\n",
       " 'c h e c k o u t',\n",
       " 'c h e q u e',\n",
       " 'c h e r r y r o a d',\n",
       " 'c h e t u',\n",
       " 'c h i p b o t',\n",
       " 'c h o o z l e',\n",
       " 'c h o r a l l y',\n",
       " 'c h r o m e',\n",
       " 'c h r o n o s c a n',\n",
       " 'c h r o n o t r a c k',\n",
       " 'c h r o n o s p h e r e',\n",
       " 'c h u r c h',\n",
       " 'c h u r c h t r a c',\n",
       " 'c h u r c h t e a m s',\n",
       " 'c h u r n 3 6 0',\n",
       " 'c h u r n r x',\n",
       " 'c i g a t i',\n",
       " 'c i n c h y',\n",
       " 'c i n c i n n a t i',\n",
       " 'c i n t r a',\n",
       " 'c i s c o',\n",
       " 'c i t r i x',\n",
       " 'c l a r a v i n e',\n",
       " 'c l a r i f a i',\n",
       " 'c l a r i f y',\n",
       " 'c l a r i o',\n",
       " 'c l a r o t y',\n",
       " 'c l a y',\n",
       " 'c l e a r',\n",
       " 'c l e a r c u t',\n",
       " 'c l e a r d 3',\n",
       " 'c l e a r d a t a',\n",
       " 'c l e a r d a s h',\n",
       " 'c l e a r d e n t',\n",
       " 'c l e a r m l',\n",
       " 'c l e a r p o i n t',\n",
       " 'c l e a r b i t',\n",
       " 'c l e a r s t o r y',\n",
       " 'c l e a r s w i f t',\n",
       " 'c l e a r w a t e r',\n",
       " 'c l e d a r a',\n",
       " 'c l e o',\n",
       " 'c l e v e r c o n t r o l',\n",
       " 'c l e v e r m a p s',\n",
       " 'c l i c d a t a',\n",
       " 'c l i c k',\n",
       " 'c l i c k h o u s e',\n",
       " 'c l i e n t f l o',\n",
       " 'c l i e n t s h a r e',\n",
       " 'c l i m a t e',\n",
       " 'c l i m e d o',\n",
       " 'c l i n c a s e',\n",
       " 'c l i n d e x',\n",
       " 'c l i n e v o',\n",
       " 'c l i n i o n',\n",
       " 'c l i n k e d',\n",
       " 'c l o n e t a b',\n",
       " 'c l o s e',\n",
       " 'c l o u d',\n",
       " 'c l o u d a l l y',\n",
       " 'c l o u d a v o c a d o',\n",
       " 'c l o u d c h e c k r',\n",
       " 'c l o u d e x t e n d',\n",
       " 'c l o u d f a c t o r y',\n",
       " 'c l o u d g u a r d',\n",
       " 'c l o u d h p t',\n",
       " 'c l o u d k e e p e r',\n",
       " 'c l o u d l e a d',\n",
       " 'c l o u d l i n u x',\n",
       " 'c l o u d m',\n",
       " 'c l o u d m a s o n r y',\n",
       " 'c l o u d m o v e r',\n",
       " 'c l o u d n i n e',\n",
       " 'c l o u d n o w',\n",
       " 'c l o u d o n e',\n",
       " 'c l o u d p a y',\n",
       " 'c l o u d q u e r y',\n",
       " 'c l o u d s d s',\n",
       " 'c l o u d s p e n d',\n",
       " 'c l o u d t a s k',\n",
       " 'c l o u d z e r o',\n",
       " 'c l o u d b a c k',\n",
       " 'c l o u d b o l t',\n",
       " 'c l o u d e r a',\n",
       " 'c l o u d e r a ,',\n",
       " 'c l o u d f l a r e',\n",
       " 'c l o u d i n g o',\n",
       " 'c l o u d s i n t e l',\n",
       " 'c l o u d s c e n e',\n",
       " 'c l o u d s f e r',\n",
       " 'c l o v e r d x',\n",
       " 'c l u m i o',\n",
       " 'c l u s t e r c o n t r o l',\n",
       " 'c l u t c h . c o',\n",
       " 'c l u v i o',\n",
       " 'c o c o',\n",
       " 'c o p i l o t',\n",
       " 'c o p l o t',\n",
       " 'c o s t a r',\n",
       " 'c o a c h a c c o u n t a b l e',\n",
       " 'c o a c h i n g',\n",
       " 'c o b a l t',\n",
       " 'c o c o p a c k e t',\n",
       " 'c o d e l o g i c ,',\n",
       " 'c o d e t o g e t h e r',\n",
       " 'c o d e t w o',\n",
       " 'c o d e a i d',\n",
       " 'c o d e n v y',\n",
       " 'c o d e r s r a n k',\n",
       " 'c o d i n g',\n",
       " 'c o d l e o',\n",
       " 'c o e f f i c i e n t',\n",
       " 'c o e u s',\n",
       " 'c o g i n i t i',\n",
       " 'c o g i t',\n",
       " 'c o g n i t i v e',\n",
       " 'c o g n i z a n t',\n",
       " 'c o h e r e n t',\n",
       " 'c o h e s i t y',\n",
       " 'c o h l e y',\n",
       " 'c o l a b o',\n",
       " 'c o l a b o r a t o r y',\n",
       " 'c o l d l y t i c s',\n",
       " 'c o l l a b i o',\n",
       " 'c o l l a b o a r d',\n",
       " 'c o l l a b o r a t e',\n",
       " 'c o l l e c t',\n",
       " 'c o l l e c t . c h a t',\n",
       " 'c o l l e c t i v',\n",
       " 'c o l l i b r a',\n",
       " 'c o l o u r',\n",
       " 'c o m',\n",
       " 'c o m a r c h',\n",
       " 'c o m e t',\n",
       " 'c o m m c a r e',\n",
       " 'c o m m e n t s',\n",
       " 'c o m m o n',\n",
       " 'c o m m u s o f t',\n",
       " 'c o m m v a u l t',\n",
       " 'c o m p s t a k',\n",
       " 'c o m p a r a b l y',\n",
       " 'c o m p e l l o n',\n",
       " 'c o m p l e a t',\n",
       " 'c o m p l i a n c e',\n",
       " 'c o m p l y 3 6 5',\n",
       " 'c o m p l y c l o u d',\n",
       " 'c o m p l y c u b e',\n",
       " 'c o m p o s a b l e',\n",
       " 'c o m p o s e',\n",
       " 'c o m p u d a t a',\n",
       " 'c o m p y l',\n",
       " 'c o n w i z e',\n",
       " 'c o n c i l i a c',\n",
       " 'c o n d e n s',\n",
       " 'c o n d r e y',\n",
       " 'c o n d u i t',\n",
       " 'c o n e i x',\n",
       " 'c o n e k t t o',\n",
       " 'c o n f i r m',\n",
       " 'c o n f l u e n t',\n",
       " 'c o n g a',\n",
       " 'c o n n e c t',\n",
       " 'c o n n e c t a l l',\n",
       " 'c o n n e c t w i s e',\n",
       " 'c o n n e c t e d s i g n',\n",
       " 'c o n n e c t i f i e r',\n",
       " 'c o n n e c t i v i t y',\n",
       " 'c o n n e c t l y . a i',\n",
       " 'c o n n e x i t y',\n",
       " 'c o n s e n t',\n",
       " 'c o n s i d e r',\n",
       " 'c o n s o l e',\n",
       " 'c o n s t e l l a',\n",
       " 'c o n t a c t',\n",
       " 'c o n t a c t b o s s',\n",
       " 'c o n t a c t d b',\n",
       " 'c o n t a c t p i g e o n',\n",
       " 'c o n t e n t',\n",
       " 'c o n t e r r a',\n",
       " 'c o n t i f y',\n",
       " 'c o n t i n u i t y',\n",
       " 'c o n t l o',\n",
       " 'c o n t r a c t z e n',\n",
       " 'c o n t r o l - m',\n",
       " 'c o n t r o l h u b',\n",
       " 'c o n t r u e n t',\n",
       " 'c o n v e r g e o n e',\n",
       " 'c o n v e r g e d',\n",
       " 'c o n v e r s u s . a i',\n",
       " 'c o n v e r t',\n",
       " 'c o n v e r t e d i n',\n",
       " 'c o n v e r t r',\n",
       " 'c o n v i c t i o n a l',\n",
       " 'c o o b y',\n",
       " ...]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make unique list of software and first word of software name\n",
    "\n",
    "df = pd.read_csv('Data_Words/Data/software_data_entities.csv', index_col=0)\n",
    "software_list = (df['name'])\n",
    "software_full_name_list = [x.split(\" \") for x in software_list]\n",
    "first_software_list = [x[0] for x in software_full_name_list]\n",
    "all_software = first_software_list + software_full_name_list\n",
    "match_software = []\n",
    "[match_software.append(x) for x in all_software if x not in match_software]\n",
    "clean_match_software = []\n",
    "clean_match_software2 = []\n",
    "try:\n",
    "    [clean_match_software.append(x.lower()) for x in match_software if x not in clean_match_software]\n",
    "except:\n",
    "    [clean_match_software2.append(\" \".join(x).lower()) for x in match_software if x not in clean_match_software2]\n",
    "\n",
    "clean_match_software2\n",
    "# sft_df = pd.DataFrame(set(clean_match_software), columns=[\"software\"])\n",
    "\n",
    "# sft_df.to_csv('Data_Words/Data/data_software_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stop words and punctuation\n",
    "stopwords = list(nlp.Defaults.stop_words)\n",
    "punctuation = list(punctuation)\n",
    "\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df2 = pd.read_csv('Data_Words/Data/categoryWords.csv', index_col=0)\n",
    "adj_words =[]\n",
    "desc = list(df2['category'])\n",
    "desc_split = [x.split(' ') for x in desc]\n",
    "\n",
    "\n",
    "for des in desc_split:\n",
    "    for i in range(0,len(des)):\n",
    "        adj_words.append(des[:i])\n",
    "        adj_words.append(des[-i:])\n",
    "clean_description_words = []\n",
    "for adj in adj_words:\n",
    "    words = filter(lambda x: x not in stopwords, adj)\n",
    "    all_d = \" \".join(words)\n",
    "    all_punc = all_d.replace\n",
    "    clean_description_words.append(all_d)\n",
    "final_words = []\n",
    "for s in clean_description_words:\n",
    "    nop = ''\n",
    "    for l in s:\n",
    "        if l not in punctuation:\n",
    "            nop += l\n",
    "    if len(nop)>0 and nop:\n",
    "        final_words.append(nop)\n",
    "    else:\n",
    "        pass\n",
    "descriptions = []\n",
    "[descriptions.append(x) for x in final_words if x not in descriptions]\n",
    "data_software_words = [x.lower() for x in descriptions]\n",
    "df_des = pd.DataFrame(data_software_words, columns=['software'])\n",
    "df_des.to_csv('Data_Words/Data/data_description_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2 = pd.read_csv('Data_Words/Data/software_data_entities.csv', index_col=0)\n",
    "adj_words =[]\n",
    "desc = list(df2['name'])\n",
    "desc_split = [x.split(' ') for x in desc]\n",
    "\n",
    "\n",
    "for des in desc_split:\n",
    "    for i in range(0,len(des)):\n",
    "        adj_words.append(des[:i])\n",
    "        adj_words.append(des[-i:])\n",
    "clean_description_words = []\n",
    "for adj in adj_words:\n",
    "    words = filter(lambda x: x not in stopwords, adj)\n",
    "    all_d = \" \".join(words)\n",
    "    all_punc = all_d.replace\n",
    "    clean_description_words.append(all_d)\n",
    "final_words = []\n",
    "for s in clean_description_words:\n",
    "    nop = ''\n",
    "    for l in s:\n",
    "        if l not in punctuation:\n",
    "            nop += l\n",
    "    if len(nop)>0 and nop:\n",
    "        final_words.append(nop)\n",
    "    else:\n",
    "        pass\n",
    "descriptions = []\n",
    "[descriptions.append(x) for x in final_words if x not in descriptions]\n",
    "data_software_words = [x.lower() for x in descriptions]\n",
    "df_des = pd.DataFrame(data_software_words, columns=['software'])\n",
    "df_des.to_csv('Data_Words/Data/data_software_words.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\' Must be legally authorized to work in the United States without the need for employer sponsorshp now or at any time in the future\\', \" Bachelor\\'s Degree in engineering, computer science, or other technical field\", \\' 5 years of experience working in data science and systems engineering in a professional services consulting environmcent or supporting a US Military Program Office\\', \\' Knowledge of US government configuration management processes\\', \\' Strong programming skills in Python, R, or similar languages\\', \\' Experience with statistical modeling and machine learning techniques, such as linear regression, decision trees, clustering, and neural networks\\', \\' Active Top Secret Clearance and SCI eligibility\\', \" Master\\'s Degree in computer science, statistics, mathematics or a related field\", \\' Familiarity with SQL and database systems\\', \\' Knowledge of data visualization tools such as Tableau, Power BI, or D3.js\\', \\' Active TS/SCI US Government Clearance\\']'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy_cleaner import processing, Cleaner\n",
    "from spacy.matcher import PhraseMatcher\n",
    "# get lists of vocab and spans\n",
    "df_sft = pd.read_csv(\"Data_Words/Data/data_software_words.csv\", index_col=0)\n",
    "sft_terms =list(df_sft['software'])\n",
    "text_all = pd.read_csv('Data_Words/Data/job_words_indeed.csv', index_col=0)\n",
    "text_line = text_all['description_sentences'][0]\n",
    "docs = nlp(text_line)\n",
    "\n",
    "\n",
    "# matcher = PhraseMatcher(nlp.vocab, attr= \"LOWER\")\n",
    "# docs = list(nlp.pipe(text_line))\n",
    "# data_clean = [[w.lemma_ for w in doc if (not w.is_stop and not w.is_punct and not w.like_num)] for doc in docs]\n",
    "# )\n",
    "# # patterns = [nlp.make_doc(text) for text in sft_terms]\n",
    "# # matcher.add(\"data_software\", patterns)\n",
    "text_line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy_cleaner in ./.venv/lib/python3.11/site-packages (3.2.1)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in ./.venv/lib/python3.11/site-packages (from spacy_cleaner) (3.7.4)\n",
      "Requirement already satisfied: spacy-lookups-data<1.1.0,>=1.0.5 in ./.venv/lib/python3.11/site-packages (from spacy_cleaner) (1.0.5)\n",
      "Requirement already satisfied: tqdm<4.67.0,>=4.66.1 in ./.venv/lib/python3.11/site-packages (from spacy_cleaner) (4.66.2)\n",
      "Requirement already satisfied: types-tqdm<5.0.0.0,>=4.66.0.5 in ./.venv/lib/python3.11/site-packages (from spacy_cleaner) (4.66.0.20240106)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (6.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (3.1.3)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->spacy_cleaner) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->spacy_cleaner) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in ./.venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->spacy_cleaner) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./.venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->spacy_cleaner) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->spacy_cleaner) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->spacy_cleaner) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->spacy_cleaner) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->spacy_cleaner) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./.venv/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->spacy_cleaner) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->spacy_cleaner) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./.venv/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->spacy_cleaner) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in ./.venv/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->spacy_cleaner) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->spacy_cleaner) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PhraseMatcher' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# def on_match(matcher, doc, id, matches):\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     print('Matched!', matches)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m matcher \u001b[38;5;241m=\u001b[39m \u001b[43mPhraseMatcher\u001b[49m(nlp\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[1;32m      5\u001b[0m matcher\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOBAMA\u001b[39m\u001b[38;5;124m\"\u001b[39m, [nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBarack Obama\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[1;32m      6\u001b[0m matcher\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEALTH\u001b[39m\u001b[38;5;124m\"\u001b[39m, [nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhealth care reform\u001b[39m\u001b[38;5;124m\"\u001b[39m), nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhealthcare reform\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PhraseMatcher' is not defined"
     ]
    }
   ],
   "source": [
    "# def on_match(matcher, doc, id, matches):\n",
    "#     print('Matched!', matches)\n",
    "import spacy\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"OBAMA\", [nlp(\"Barack Obama\")])\n",
    "matcher.add(\"HEALTH\", [nlp(\"health care reform\"), nlp(\"healthcare reform\")])\n",
    "doc = nlp(\n",
    "    \"Barack Obama urges Congress to find courage to defend his healthcare reforms\"\n",
    ")\n",
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Spacy nlp functions\"\"\"\n",
    "\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "stopwords = list(nlp.Defaults.stop_words)\n",
    "\n",
    "def spacy_proper(doc):\n",
    "    \"\"\"Takes string as input and returns a list of Proper Nouns\"\"\"\n",
    "    pn_list = []\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == 'PROPN':\n",
    "            pn_list.append(tok.text)\n",
    "        else:\n",
    "            pass\n",
    "    return pn_list\n",
    "\n",
    "def sentence_parse_proper(sentences):\n",
    "    \"\"\"Parses list of sentences and returns list of proper nouns\"\"\"\n",
    "    col_lists = []\n",
    "    try:\n",
    "        for sentence in sentences:\n",
    "            ss= sentence.strip()\n",
    "            doc = nlp(ss)\n",
    "            pn = spacy_proper(doc)\n",
    "            col_lists.append(pn)\n",
    "    except ValueError:\n",
    "        col_lists.append([])\n",
    "    return set(chain.from_iterable(col_lists))\n",
    "\n",
    "def pattern_lower(csv_file, column_name):\n",
    "    \"\"\"Formats column from csv file into patterns for matching\"\"\"\n",
    "    pattern_list = []\n",
    "    df = pd.read_csv(csv_file)\n",
    "    word_list = list(df[column_name])\n",
    "    clean_word_list = [str(t).lower().strip() for t in word_list if t is not np.nan]\n",
    "    split_list = [t.split() for t in clean_word_list ]\n",
    "    for i in range(len(split_list)):\n",
    "        words = []\n",
    "        sentence = split_list[i]\n",
    "        for w in sentence:\n",
    "            pattern = dict(LOWER = str(w))\n",
    "            words.append(pattern)\n",
    "        pattern_list.append(words)\n",
    "    # print(f'{column_name} now contains {len(pattern_list)} keywords')\n",
    "    return pattern_list\n",
    "\n",
    "\n",
    "def data_word_match(sentence, csv_file, column_name):\n",
    "    \"\"\"Matches phrase patterns\"\"\"\n",
    "    matcher=Matcher(nlp.vocab)\n",
    "    word_patterns = pattern_lower(csv_file, column_name)\n",
    "    matcher.add(column_name, word_patterns, greedy='FIRST')\n",
    "    doc = nlp(sentence)\n",
    "    matches = matcher(doc)\n",
    "    words = []\n",
    "    for start, end in matches:  ##match_id, not used\n",
    "        span = doc[start:end]\n",
    "        words.append(span.text)\n",
    "    return list(words)\n",
    "\n",
    "# TODO: use lemmatization to match base words instead of exact # [fixme]\n",
    "\n",
    "def sentence_parse_data_words (sentences, csv_file, column_name):\n",
    "    \"\"\"Takes string as input and returns a list of Proper Nouns\"\"\"\n",
    "    words_lists = []\n",
    "    try:\n",
    "        for sentence in sentences:\n",
    "            words = data_word_match(sentence,csv_file, column_name)\n",
    "            words_lists.append(words)\n",
    "    except ValueError:\n",
    "        words_lists.append([])\n",
    "    return set(chain.from_iterable(words_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Spacy nlp functions\"\"\"\n",
    "\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "stopwords = list(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Test_Job.txt\", \"r\") as f:\n",
    "    doc = nlp(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ELT',\n",
       " 'SQL',\n",
       " 'dbt',\n",
       " 'Act',\n",
       " 'SQL',\n",
       " 'dbt',\n",
       " 'experience',\n",
       " 'Cloud',\n",
       " 'Snowflake',\n",
       " 'BigQuery',\n",
       " 'Fivetran',\n",
       " 'Stitch',\n",
       " 'Hightouch',\n",
       " 'Census',\n",
       " 'Business',\n",
       " 'BI',\n",
       " 'Looker',\n",
       " 'Tableau']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spacy_proper(doc):\n",
    "    \"\"\"Takes string as input and returns a list of Proper Nouns\"\"\"\n",
    "    pn_list = []\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == \"PROPN\":\n",
    "            pn_list.append(tok.text)\n",
    "        else:\n",
    "            pass\n",
    "    return pn_list\n",
    "\n",
    "\n",
    "spacy_proper(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sentence_parse_proper(sentences):\n",
    "    \"\"\"Parses list of sentences and returns list of proper nouns\"\"\"\n",
    "    col_lists = []\n",
    "    try:\n",
    "        for sentence in sentences:\n",
    "            ss = sentence.strip()\n",
    "            doc = nlp(ss)\n",
    "            pn = spacy_proper(doc)\n",
    "            col_lists.append(pn)\n",
    "    except ValueError:\n",
    "        col_lists.append([])\n",
    "    return set(chain.from_iterable(col_lists))\n",
    "\n",
    "\n",
    "def pattern_lower(csv_file, column_name):\n",
    "    \"\"\"Formats column from csv file into patterns for matching\"\"\"\n",
    "    pattern_list = []\n",
    "    df = pd.read_csv(csv_file)\n",
    "    word_list = list(df[column_name])\n",
    "    clean_word_list = [str(t).lower().strip() for t in word_list if t is not np.nan]\n",
    "    split_list = [t.split() for t in clean_word_list]\n",
    "    for i in range(len(split_list)):\n",
    "        words = []\n",
    "        sentence = split_list[i]\n",
    "        for w in sentence:\n",
    "            pattern = dict(LOWER=str(w))\n",
    "            words.append(pattern)\n",
    "        pattern_list.append(words)\n",
    "    # print(f'{column_name} now contains {len(pattern_list)} keywords')\n",
    "    return pattern_list\n",
    "\n",
    "\n",
    "def data_word_match(sentence, csv_file, column_name):\n",
    "    \"\"\"Matches phrase patterns\"\"\"\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    word_patterns = pattern_lower(csv_file, column_name)\n",
    "    matcher.add(column_name, word_patterns, greedy=\"FIRST\")\n",
    "    doc = nlp(sentence)\n",
    "    matches = matcher(doc)\n",
    "    words = []\n",
    "    for start, end in matches:  ##match_id, not used\n",
    "        span = doc[start:end]\n",
    "        words.append(span.text)\n",
    "    return list(words)\n",
    "\n",
    "\n",
    "# TODO: use lemmatization to match base words instead of exact # [fixme]\n",
    "\n",
    "\n",
    "def sentence_parse_data_words(sentences, csv_file, column_name):\n",
    "    \"\"\"Takes string as input and returns a list of Proper Nouns\"\"\"\n",
    "    words_lists = []\n",
    "    try:\n",
    "        for sentence in sentences:\n",
    "            words = data_word_match(sentence, csv_file, column_name)\n",
    "            words_lists.append(words)\n",
    "    except ValueError:\n",
    "        words_lists.append([])\n",
    "    return set(chain.from_iterable(words_lists))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
